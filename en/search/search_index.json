{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"designs/","title":"Log Hub","text":"<p>Important</p> <p>If you are looking for the user/implementation guide of Log Hub, please refer to Implementation Guide.  The content here are design documentations of Log Hub solution. We have a plan to open source all our designs (including historical designs) on this site. However, we do NOT guarantee  all documentations here are implemented in the Log Hub solution. </p> <p>Log Hub is an AWS Solution simplifies the build of log analytics pipelines on top of Amazon OpenSearch Service, which helps customers to improve engineering efficiency on log ingestion, log processing and log visualization. The Log Hub solution provides to customers, as a complementary of Amazon OpenSearch Service, capabilities of centralized log ingestion across multiple regions and account, one-click creation of codeless log processors and templated dashboards for visualization. With the unified web console, the Log Hub solution allows customers to create end-to-end log analytics workloads by automating the orchestration of different AWS services within minutes.</p>"},{"location":"designs/#resources","title":"Resources","text":"<ul> <li>GitHub Repo</li> <li>Feature Request/Bug Report</li> </ul>"},{"location":"designs/FAQ/","title":"Internal FAQ","text":""},{"location":"designs/FAQ/#will-the-solution-support-self-hosted-elasticsearch-domain-in-the-future","title":"Will the solution support self-hosted Elasticsearch domain in the future?","text":"<p>There is no clear answer for this question yet. We have heard from customers that the AOS price is too high comparing  to self-hosted Elasticsearch domain. Because of this, we have added this feature in our backlog. However, we need more  information to prioritize our backlogs. Currently, it is in low priority.</p>"},{"location":"designs/assets/","title":"Assets","text":"<p>Here are some (not all) assets that help in building the Log Hub solution. We appreciate their great contribution to those blogs, open source products. </p>"},{"location":"designs/assets/#blogs","title":"Blogs","text":"<ul> <li>Configuring and authoring Kibana dashboards</li> <li>Query and analyze Amazon S3 data with the new Amazon Athena plugin for Grafana</li> <li>Query data in Amazon OpenSearch Service using SQL from Amazon Athena</li> <li>Build an observability solution using managed AWS services and the OpenTelemetry standard</li> <li>Deploy a dashboard for AWS WAF with minimal effort</li> <li>Log Analytics on AWS</li> <li>\u5728 AWS \u4e2d\u56fd\u533a\u5bf9 Amazon Elasticsearch Kibana \u8fdb\u884c\u8eab\u4efd\u8ba4\u8bc1\u7684\u89e3\u51b3\u65b9\u6848</li> </ul>"},{"location":"designs/assets/#github-projects","title":"GitHub projects","text":"<ul> <li>awslabs/centralized-logging</li> <li>siem-on-amazon-opensearch-service</li> <li>mingrammer/flog</li> <li>aws-samples/amazon-elasticsearch-service-with-cognito</li> <li>jkeczan/aws-api-gateway-elastic-search-proxy</li> </ul>"},{"location":"designs/concepts/","title":"Concepts","text":"<p>We created a couple of concepts to keep us at the same page. The following session will explain the definition of each concept.</p>"},{"location":"designs/concepts/#solution-components","title":"Solution Components","text":"<ul> <li> <p>Search Engine. The Search Engine is a bottom layer to index the log information and provide CRUD (Create, Read, Update,   Delete) capability to the other applications. In this system, the Search Engine represents Amazon Elasticsearch (AES)   cluster by default.</p> </li> <li> <p>Log Visual. It is a piece of software used to build up visuals, metrics and dashboard. In this system, the Log Visual   represents the Kibana associated with the AES cluster.</p> </li> <li> <p>Log Visual Template. A Log Visual Template is a standard configuration of commonly used applications (or AWS native   service) in Log Visual. For example, a standard dashboard template for Nginx, Apache, MySQL, or CloudFront.</p> </li> <li> <p>Log Buffer. A middle layer between Log Agent and AES. We introduce this layer to protect the engine layer from   being overwhelmed.</p> </li> <li> <p>Log Jobs. A Layer between Log Buffer Layer and Search Engine. In this layer, the users can perform some data cleanup   using some additional compute resource. For example, Lambda, Glue.</p> </li> <li> <p>Log Store. A storage media to keep log data. </p> </li> <li> <p>Instance Agent. An Instance Agent is a piece of software installed on EC2 instances (or on-premise instances) which can be   used to collect log and report to Amazon Elasticsearch (AES), Log Buffer or Log Storage.</p> </li> <li> <p>Container Agent. A daemon agent which being used on ECS/EKS cluster to collect log and report to Search Engine,   Cache Layer or Storage Layer.</p> </li> <li> <p>Mobile SDK. An SDK embedded in the mobile (iOS, Android, Web) client. Customers can use this SDK to authenticate against   AWS and send log, click stream to AWS.</p> </li> <li> <p>Configuration Center. It is a portal deployed in the customer's AWS account protected by Cognito User Pool or   OpenID Connect Provider. Customers can manage/import Search Engine, install/configure log agents, backup data and others.</p> </li> <li> <p>Log Format Config. A log format template is a configuration file for log agent to know the format of the   log, the location of the log file, the destination of the log, the local log processor, and others.</p> </li> <li> <p>Log Collector for Services. A component used to extract logs from AWS native services and send to   Log Buffer or Log Engine. For example, Log Collector for Lambda is a component used to extract data from CloudWatch   Logs and upload to AES or Kinesis. Log Collector for CloudFront is a component extract logs from S3 and upload to AES   or Kinesis. Log Collector for Lambda@Edge can extract logs from CloudWatch Logs in different regions and upload.</p> </li> <li> <p>Log Monitor. A component to create alarm and send notifications. In this system, we use the Kibana built-in feature   together with SNS. We also extend SNS notification target to support WeChat Enterprise account and DingTalk.</p> </li> <li> <p>Multi-tenant. A fine-grained access control to log data.</p> </li> <li> <p>Workload Simulator. A typical workload architect with application running on AWS. For example, a 3-tier web    application. This is used for customers to get sample experience of this solution.</p> </li> <li> <p>Traffic Generator. A piece of software used to generate web traffic, this will cause the simulated workload    to generate logs.</p> </li> </ul>"},{"location":"designs/concepts/#accounts","title":"Accounts","text":"<ul> <li> <p>Main Account. The main AWS account which contains the underlying infrastructure like Search Engine, Log Buffer,    Log Jobs, Log Storage, Configuration Center and others. All logs are sent to this account for further analysis and    storage.</p> </li> <li> <p>Sub Account. The other AWS accounts where your application are deployed and need to be sent to the centralized logging    platform.</p> </li> </ul>"},{"location":"designs/concepts/#solution-assets","title":"Solution Assets","text":"<ul> <li> <p>Deliverables. Amazon CloudFormation templates or AWS CDK package used to create a solution component or a combination    of solution components. The rest assets like Lambda code will be hosted in public S3 buckets owned by Solutions Builder team.</p> </li> <li> <p>Implementation Guide. A user manual gives guidance to the customers how to use the solution. It includes a step-by-step    Getting Started guide.</p> </li> <li> <p>Workshop Portal. A portal hosting the associated solution workshop. The workshop content can be used for the field    team to organize an offline customer facing workshop.</p> </li> </ul>"},{"location":"designs/log-analytics-pipeline-service/","title":"Service Log","text":""},{"location":"designs/log-analytics-pipeline-service/#amazon-s3-access-log","title":"Amazon S3 Access Log","text":""},{"location":"designs/log-analytics-pipeline-service/#amazon-cloudtrail-log","title":"Amazon CloudTrail Log","text":""},{"location":"designs/user-stories/","title":"User Stories","text":""},{"location":"designs/user-stories/#deploy-configuration-center","title":"Deploy Configuration Center","text":""},{"location":"designs/user-stories/#install-instance-agent","title":"Install Instance Agent","text":"<p>EC2 instances launched from EC2 Quick Start comes with SSM Agent installed. However, it is not associated with an appropriate instance profile which enable them to connect to the SSM control panel. Once the instance profile associated with EC2 instances, we can use the SSM Run Command to install Log Agent to report logs to Elasticsearch.</p>"},{"location":"designs/user-stories/#install-container-agent","title":"Install Container Agent","text":""},{"location":"designs/user-stories/#import-search-engine","title":"Import Search Engine","text":"<p>s</p>"},{"location":"designs/UI/help-panel/","title":"Help Panel","text":"<p>Help panel is a place to offer more information to customers. The Sketch file will not provide help panel information. We consolidate all help panel information in this PRD. The following is an example of Help Panel and its description.</p>"},{"location":"designs/UI/help-panel/#example","title":"Example","text":"Section Content Notes Title Help panel title Body This is a paragraph with some bold text and also some italic text.h4 section headerCode can be formatted as lines of code or blocks of code.<code>&lt;this is a block of code&gt;</code>h4 section headerCode can be formated as lines of code or blocks of code.<code>&lt;This is a block of code&gt;</code> Learn more First link to the documentationSecond link to the documentation"},{"location":"designs/UI/help-panel/#domain-detail","title":"Domain detail","text":""},{"location":"designs/UI/help-panel/#access-proxy","title":"Access Proxy","text":"Section Content Notes Title Access Proxy Body Access Proxy creates a Nginx based proxy (behind Application Load Balancer) which allows you to access the OpenSearch Dashboards through Internet.Prerequisites1. Domain name2. The domain associated SSL certificate in Amazon Certificate Manager (ACM)3. A EC2 public key Learn more Create a Access Proxy"},{"location":"designs/UI/help-panel/#alarms","title":"Alarms","text":"Section Content Notes Title Alarms Body Amazon OpenSearch provides a set of recommended CloudWatch alarms, Log Hub can help customers to create the alarms automatically, and sent notification to your email (or SMS) via SNS. Learn more Create OpenSearch Alarms"},{"location":"designs/UI/help-panel/#log-processing","title":"Log Processing","text":"Section Content Notes Title Log Processing Body Log Hub will provision Lambda (or other compute resource) to process logs using these networking configurations. You can specify the log processing networking layer when import OpenSearch domains. NoteThe log processing layer has access to the OpenSearch domain. Learn more Import OpenSearch domain"},{"location":"designs/UI/help-panel/#import-domain","title":"Import Domain","text":""},{"location":"designs/UI/help-panel/#networking-creation-creation-method","title":"Networking creation - Creation method","text":"Section Content Notes Title Creation method Body When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. Log Hub will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain. AutomaticLog Hub will detect if there is a need to create a VPC Peering Connection. If needed, Log Hub will automatically create a VPC Peering Connection, update route table, and update the security group of OpenSearch domain.ManualManually specify the Log Processing Layer networking information. You may need to create VPC Peering Connection, update route table and security group of OpenSearch domain. Learn more Import OpenSearch domain"},{"location":"designs/UI/help-panel/#log-processing-network","title":"Log processing network","text":"Section Content Notes Title Log processing network Body When import OpenSearch domains, you need to specify the networking configuration associated with the Log Processing Layer. Log Hub will automatically place Lambda (or other compute resource) in this layer. The Log Processing Layer must have access to the OpenSearch domain. S3 Service accessBy default, Log Hub will output error logs to Amazon S3. Please guarantee the log processing layer has network access to S3. You can do it by place the log processing layer in public subnets, use AWS PrivateLink for Amazon S3 or via NAT Gateways.CloudWatch Logs accessMany AWS services output service logs to CloudWatch Logs. If you use Log Hub to ingest service logs. Please guarantee the log processing layer has network access to CloudWatch Logs. Kinesis Data Streams accessApplication logs are sent to Kinesis Data Streams in Log Hub. Please guarantee the log processing layer has networking access to Kinesis Data Streams. Learn more"},{"location":"designs/UI/help-panel/#service-log-ingestion","title":"Service Log ingestion","text":""},{"location":"designs/UI/help-panel/#creation-method","title":"Creation method","text":"Section Content Notes Title Log enabling Body Log Hub can automatically detect the log location, or you can specify the log location manually. AutomaticLog Hub will automatically detect the log location of the selected AWS service. If needed, it will enable the service log and save to a centralized log bucket.ManualManually input the AWS service source and its log location . Log Hub will read logs from the location you specified. Learn more"},{"location":"designs/UI/help-panel/#sample-dashboard","title":"Sample dashboard","text":"Section Content Notes Title Sample dashboard Body Log Hub will insert a preconfigured dashboard into the OpenSearch domain if Yes being selected. The dashboard name will be consist with your index name. Learn more"},{"location":"designs/UI/help-panel/#log-lifecycle","title":"Log lifecycle","text":"Section Content Notes Title Log lifecycle Body Log Hub will insert an Index State Management (ISM) into the OpenSearch domain. The life cycle will periodically move your indices in OpenSearch to save cost. Learn more Index State Management"},{"location":"designs/UI/help-panel/#log-config","title":"Log Config","text":""},{"location":"designs/UI/help-panel/#log-path","title":"Log Path","text":"Section Content Notes Title Log Path Body Specify the log file locations. If you have mutliple locations, please write all the locations and split using ','.  e.g. <code>/var/log/app1/*.log,/var/log/app2/*.log</code>. Learn more"},{"location":"designs/UI/help-panel/#nginx-log-format","title":"Nginx Log Format","text":"Section Content Notes Title Nginx Log Format Body Nginx capture detailed information about errors and request in log files. You can find the log format configuration in Nginx configuration file, such as the <code>/etc/nginx/nginx.conf</code> file.  The log format directive starts with <code>log_format</code>. Learn more Configuring Logging in Nginx"},{"location":"designs/UI/help-panel/#apache-log-format","title":"Apache Log Format","text":"Section Content Notes Title Apache HTTP Server Log Format Body Apache HTTP Server capture detailed information about errors and request in log files. You can find the log format configuration in Apache HTTP Server configuration file, such as the <code>/etc/httpd/conf/httpd.conf</code> file.  The log format directive starts with <code>LogFormat</code>. Learn more Apache HTTP Server Log Files"},{"location":"designs/UI/help-panel/#regular-expression","title":"Regular Expression","text":"Section Content Notes Title RegEx Log Format Body Log Hub uses custom Ruby Regular Expression to parse logs.  It supports both single-line log format and mutliple input format. Write the regular expression in Rubular to validate first and input the value here. Learn more Regular ExpressionRubular: A Rudy-based reular expression editorRegular Expression in Fluent Bit"},{"location":"designs/UI/help-panel/#application-pipeline","title":"Application Pipeline","text":""},{"location":"designs/UI/help-panel/#creation-method_1","title":"Creation Method","text":"Section Content Notes Title Instance Group Creation Body Create a new instance group, or choose an existing Instance Group created before. Learn more Instance Group"},{"location":"designs/app-log/api-design/","title":"Application Log API Design","text":""},{"location":"designs/app-log/api-design/#log-config-apis","title":"Log Config APIs","text":"<p>The following operations are available in the solution's Log Config APIs.</p>"},{"location":"designs/app-log/api-design/#create-log-config","title":"Create Log Config","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description confName String Yes The name of the log configuration. The name must be unique, and can only contains lower case letters and -. logType enum Yes JSON, Apache, Nginx, SingleLineText, MultiLineText. timeKey String No Time key in the log. timeOffset String No Timezone offset for the log multilineLogParser enum No JAVA_SPRING_BOOT. userLogFormat String No The log format configuration. For instance, the log format configuration of Apache. e.g. LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b\" common. userSampleLog String No Sampled log. regularExpression String No When the log type you select is SingleLineText, MultiLineText, you need to define a regular expression to parse the log. regularSpecs K-V No To be used to parse the log field type, we will create an index template for the search engine based on this. timeRegularExpression String No When the time key is specified, you need to define a regular expression to parse the time format. processorFilterRegex K-V No Filter details, such as filter key and condition etc. <p>Simple Request &amp; Response:</p> <pre><code>query example{\n  createLogConf(\n    confName: \"nginx-log\",\n    logType: \"Nginx\",\n    userSampleLog: \"127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \\\"GET / HTTP/1.1\\\" 200 3520 \\\"-\\\" \\\"curl/7.79.1\\\" \\\"-\\\"\",\n    userLogFormat: \"log_format%20%20main%20%20...*\",\n    regularSpecs: [],\n    timeRegularExpression: \"\",\n    processorFilterRegex: {\n      enable: true,\n      filters: [\n        {\n          key: \"status\",\n          condition: \"Include\",\n          value: \"200\"\n        }\n      ]\n    }\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createLogConf\": \"41848bb3-f48a-4cdd-b0af-861d4be768ca\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#update-log-config","title":"Update Log Config","text":"<p>Type: Mutation</p> <p>Description: Update a log configuration.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) confName String Yes The name of the log configuration. The name must be unique, and can only contains lower case letters and -. logType enum Yes JSON, Apache, Nginx, SingleLineText, MultiLineText. timeKey String No Time key in the log. timeOffset String No Timezone offset for the log multilineLogParser enum No JAVA_SPRING_BOOT. userLogFormat String No The log format configuration. For instance, the log format configuration of Apache. e.g. LogFormat \"%h %l %u %t \\\"%r\\\" %&gt;s %b\" common. userSampleLog String No Sampled log. regularExpression String No When the log type you select is SingleLineText, MultiLineText, you need to define a regular expression to parse the log. regularSpecs K-V No To be used to parse the log field type, we will create an index template for the search engine based on this. timeRegularExpression String No When the time key is specified, you need to define a regular expression to parse the time format. processorFilterRegex K-V No Filter details, such as filter key and condition etc. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n  updateLogConf(\n    id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\",\n    confName: \"my-nginx\",\n    logType: \"Nginx\",\n    userSampleLog: \"127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \\\"GET / HTTP/1.1\\\" 200 3520 \\\"-\\\" \\\"curl/7.79.1\\\" \\\"-\\\"\",\n    userLogFormat: \"log_format%20%20main%20%20...*\",\n    regularSpecs: [],\n    timeRegularExpression: \"\",\n    processorFilterRegex: {\n      enable: true,\n      filters: [\n        {\n          key: \"status\",\n          condition: \"Include\",\n          value: \"200\"\n        }\n      ]\n    }\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"updateLogConf\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>confName already exists</li> </ul> <pre><code>{\n    \"data\": {\n        \"updateLogConf\": null\n    },\n    \"errors\": [{\n        \"path\": [\n            \"updateLogConf\"\n        ],\n        \"data\": null,\n        \"errorType\": \"Lambda:Unhandled\",\n        \"errorInfo\": null,\n        \"locations\": [{\n            \"line\": 3,\n            \"column\": 3,\n            \"sourceName\": null\n        }],\n        \"message\": \"confName already exists\"\n    }]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-log-config","title":"Delete Log Config","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteLogConf(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteLogConf\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteLogConf\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteLogConf\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-log-configs","title":"List Log Configs","text":"<p>Type: Query</p> <p>Description: List all Log Configs</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    listLogConfs (page: 1, count: 10) {\n        logConfs {\n            id\n            confName\n            logType\n            syslogParser\n            createdDt\n            status\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listLogConfs\": {\n            \"logConfs\": [\n                {\n                    \"id\": \"b942da74-f755-499a-855e-12c43267a6c0\",\n                    \"confName\": \"my-nginx\",\n                    \"logType\": \"Nginx\",\n                    \"syslogParser\": null,\n                    \"createdDt\": \"2022-10-30T03:54:38Z\",\n                    \"status\": \"ACTIVE\"\n                },\n                ...\n            ],\n            \"total\": 4\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-log-config-details","title":"Get Log Config Details","text":"<p>Type: Query</p> <p>Description: Get details of a Log Config.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Config Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getLogConf (id: \"62c9a8c5-eb43-4d25-b94f-941848525645\") {\n        id\n        confName\n        logType\n        timeKey\n        timeOffset\n        createdDt\n        userLogFormat\n        userSampleLog\n        regularExpression\n        timeRegularExpression\n        regularSpecs {\n            key\n            type\n            format\n        }\n        processorFilterRegex {\n            enable\n            filters {\n                key\n                condition\n                value\n            }\n        }\n        status\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getLogConf\": {\n            \"id\": \"62c9a8c5-eb43-4d25-b94f-941848525645\",\n            \"confName\": \"myapp\",\n            \"logType\": \"SingleLineText\",\n            \"timeKey\": \"time_local\",\n            \"timeOffset\": \"+0000\",\n            \"createdDt\": \"2022-11-30T02:48:27Z\",\n            \"userLogFormat\": \"%28%3F%3Cremote_addr%3E%5CS%2B%29%5Cs*-%5Cs*%28%....*\",\n            \"userSampleLog\": \"...\",\n            \"regularExpression\": \"%28%3F%3Cremote_addr%3E%5CS%2B%29%5Cs*-%5Cs*%28%....*\",\n            \"timeRegularExpression\": \"\",\n            \"regularSpecs\": [\n                {\n                    \"key\": \"remote_addr\",\n                    \"type\": \"text\",\n                    \"format\": null\n                },\n                {\n                    \"key\": \"remote_user\",\n                    \"type\": \"text\",\n                    \"format\": null\n                },\n                {\n                    \"key\": \"time_local\",\n                    \"type\": \"text\",\n                    \"format\": \"%d/%b/%Y:%H:%M:%S\"\n                },\n                ...\n            ],\n            \"processorFilterRegex\": {\n                \"enable\": true,\n                \"filters\": [\n                    {\n                        \"key\": \"status\",\n                        \"condition\": \"Include\",\n                        \"value\": \"200\"\n                    }\n                ]\n            },\n            \"status\": \"ACTIVE\"\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#instance-group-apis","title":"Instance Group APIs","text":"<p>The following operations are available in the solution's Instance Group APIs.</p>"},{"location":"designs/app-log/api-design/#create-instance-group","title":"Create Instance Group","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description groupName String Yes The name of the log group. The name must be unique, and can only contains lower case letters and -. instanceSet String[] Yes EC2 Instance Id set <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    createInstanceGroup(groupName: \"nginx-webgrp\", instanceSet: [\"web1\", \"web2\"])\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createInstanceGroup\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-instance-group","title":"Delete Instance Group","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteInstanceGroup(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteInstanceGroup\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteInstanceGroup\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteInstanceGroup\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-instance-groups","title":"List Instance Groups","text":"<p>Type: Query</p> <p>Description: List all Instance Groups</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listInstanceGroups(count: 10, page: 1) {\n        total\n        instanceGroups {\n            id\n            accountId\n            region\n            groupName\n            groupType\n            instanceSet\n            createdDt\n            status\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listInstanceGroups\": {\n            \"total\": 1,\n            \"instanceGroups\": [{\n                \"createdDt\": \"2021-11-06T12:28:52.041408\",\n                \"groupName\": \"fsf1\",\n                \"groupType\": \"ASG\",\n                \"id\": \"1089057b-888b-4794-b797-fef943adccf0\",\n                \"instanceSet\": [\n                    \"1\",\n                    \"2\"\n                ]\n            }]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-instance-group-details","title":"Get Instance Group Details","text":"<p>Type: Query</p> <p>Description: Get details of a Log Group.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getInstanceGroup(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\") {\n    createdDt\n        groupName\n        id\n        instanceSet\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getInstanceGroup\": {\n    \"createdDt\": \"2021-11-06T12:28:52.041408\",\n        \"groupName\": \"fsf1\",\n        \"id\": \"1089057b-888b-4794-b797-fef943adccf0\",\n        \"instanceSet\": [\n                \"1\",\n                \"2\"\n        ]\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-instances","title":"List Instances","text":"<p>Type: Query</p> <p>Description: If you specify one or more managed node IDs, it returns information for those managed nodes.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description nextToken String No The token for the next set of items to return. (You received this token from a previous call.) maxResults Int No 10 The maximum number of items to return for this call. The call also returns a token that you can specify in a subsequent call to get the next set of results. instanceSet String[] No 1 The ID of the managed instance should be retrieved <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n        listInstances(maxResults: 10, instanceSet: [\"i-0bbf9209068ced7ed\"]) {\n        instances {\n            computerName\n            id\n            ipAddress\n            platformName\n            name\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listInstances\": {\n            \"instances\": [{\n                \"computerName\": \"ip-172-31-44-205.us-west-2.compute.internal\",\n                \"id\": \"i-0bbf9209068ced7ed\",\n                \"ipAddress\": \"172.31.44.205\",\n                \"platformName\": \"CentOS Linux\",\n                \"name\": \"Bastion\"\n            }]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-log-agent-status","title":"Get Log Agent Status","text":"<p>Type: Query</p> <p>Description: Get Fluent Bit installation status.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description instanceId String Yes The ID of the managed instance should be retrieved region String No AWS region accountId String No AWS account ID <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n        getLogAgentStatus(instanceId: \"i-022c5110c4e3226bb\")\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getLogAgentStatus\": \"Online/Offline\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#application-log-pipelines-apis","title":"Application Log Pipelines APIs","text":"<p>The following operations are available in the solution's Application (App) Log Pipelines APIs.</p>"},{"location":"designs/app-log/api-design/#create-app-log-pipeline","title":"Create App Log Pipeline","text":"<p>Type: Mutation</p> <p>Description: Create an application log pipeline</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description aosParams K-V Yes Amazon OpenSearch related parameters. bufferType string Yes Type of buffer (e.g. S3, KDS etc). bufferParams List Yes Buffer related parameters. force boolean Yes Force to create pipeline when conflict detected. tags List No Custom tags. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n    createAppPipeline(\n        aosParams: {\n            coldLogTransition: 0, \n            failedLogBucket: \"backup-bucket\", \n            domainName: \"dev\", \n            engine: OpenSearch, \n            indexPrefix: \"my-index\", \n            logRetention: 180, \n            opensearchArn: \"arn:aws:es:us-west-2:123456789012:domain/dev\", \n            opensearchEndpoint: \"vpc-dev-xxx.us-west-2.es.amazonaws.com\", \n            replicaNumbers: 1, \n            shardNumbers: 5, \n            vpc: {\n                privateSubnetIds: \"subnet-1234,subnet-5678\", \n                publicSubnetIds: \"\", \n                securityGroupId: \"sg-1234\", \n                vpcId: \"vpc-0123\"\n                }, \n            warmLogTransition: 0\n        }, \n        bufferType: S3, \n        bufferParams: [\n            {paramKey: \"logBucketName\", paramValue: \"log-bucket\"}, \n            {paramKey: \"logBucketPrefix\", paramValue: \"AppLogs/my-index/year=%Y/month=%m/day=%d\"}, \n            {paramKey: \"defaultCmkArn\", paramValue: \"arn:aws:kms:us-west-2:123456789012:key/1dbbdae3-3448-4890-b956-2b9b36197784\"},\n            {paramKey: \"maxFileSize\", paramValue: \"50\"},\n            {paramKey: \"uploadTimeout\", paramValue: \"60\"},\n            {paramKey: \"compressionType\", paramValue: \"gzip\"}\n        ],\n        force: false, \n        tags: [{key: \"hello\", value: \"world\"}]\n    )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createAppPipeline\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-app-log-pipeline","title":"Delete App Log Pipeline","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Log Group Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n    deleteAppPipeline(id: \"41848bb3-f48a-4cdd-b0af-861d4be768ca\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAppPipeline\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"deleteAppPipeline\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"deleteAppPipeline\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-app-log-pipelines","title":"List App Log Pipelines","text":"<p>Type: Query</p> <p>Description: List all application log pipelines</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listAppPipelines(count: 10, page: 1) {\n        appPipelines {\n            id\n            bufferType\n            bufferParams {\n                paramKey\n                paramValue\n            }\n            aosParams {\n                opensearchArn\n                domainName\n                indexPrefix\n                warmLogTransition\n                coldLogTransition\n                logRetention\n                shardNumbers\n                replicaNumbers\n                engine\n            }\n            createdDt\n            status\n            bufferAccessRoleArn\n            bufferAccessRoleName\n            bufferResourceName\n            bufferResourceArn\n            tags {\n                key\n                value\n            }\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listAppPipelines\": {\n            \"appPipelines\": [\n                {\n                    \"id\": \"67409286-2672-413f-b477-dc1f4d7966d4\",\n                    \"bufferType\": \"S3\",\n                    \"bufferParams\": [\n                        {\n                            \"paramKey\": \"logBucketName\",\n                            \"paramValue\": \"log-bucket\"\n                        },\n                        {\n                            \"paramKey\": \"logBucketPrefix\",\n                            \"paramValue\": \"AppLogs/my-index/year=%Y/month=%m/day=%d\"\n                        },\n                        {\n                            \"paramKey\": \"defaultCmkArn\",\n                            \"paramValue\": \"arn:aws:kms:eu-west-1:123456789012:key/9619ed02-b533-4d49-91de-3dd8efa11135\"\n                        },\n                        {\n                            \"paramKey\": \"maxFileSize\",\n                            \"paramValue\": \"50\"\n                        },\n                        {\n                            \"paramKey\": \"uploadTimeout\",\n                            \"paramValue\": \"60\"\n                        },\n                        {\n                            \"paramKey\": \"compressionType\",\n                            \"paramValue\": \"gzip\"\n                        }\n                    ],\n                    \"aosParams\": {\n                        \"opensearchArn\": \"arn:aws:es:eu-west-1:123456789012:domain/dev\",\n                        \"domainName\": \"dev\",\n                        \"indexPrefix\": \"my-index\",\n                        \"warmLogTransition\": 0,\n                        \"coldLogTransition\": 0,\n                        \"logRetention\": 180,\n                        \"shardNumbers\": 5,\n                        \"replicaNumbers\": 1,\n                        \"engine\": \"OpenSearch\"\n                    },\n                    \"createdDt\": \"2022-10-30T03:03:56Z\",\n                    \"status\": \"ACTIVE\",\n                    \"bufferAccessRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-AppPipe-824f1-BufferAccessRoleDF53FD85-1ME7KUUVZVFTD\",\n                    \"bufferAccessRoleName\": \"LogHub-AppPipe-824f1-BufferAccessRoleDF53FD85-1ME7KUUVZVFTD\",\n                    \"bufferResourceName\": \"log-bucket\",\n                    \"bufferResourceArn\": \"arn:aws:s3:::log-bucket\",\n                    \"tags\": [\n                        {\n                            \"key\": \"hello\",\n                            \"value\": \"world\"\n                        }\n                    ]\n                },\n                ...\n            ],\n            \"total\": 3\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-app-log-pipeline-details","title":"Get App Log Pipeline Details","text":"<p>Type: Query</p> <p>Description: Get details of an application log pipeline.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes App Pipeline Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getAppPipeline (id: \"67409286-2672-413f-b477-dc1f4d7966d4\") {\n        id\n        bufferType\n        bufferParams {\n            paramKey\n            paramValue\n        }\n        aosParams {\n            opensearchArn\n            domainName\n            indexPrefix\n            warmLogTransition\n            coldLogTransition\n            logRetention\n            shardNumbers\n            replicaNumbers\n            engine\n        }\n        createdDt\n        status\n        bufferAccessRoleArn\n        bufferAccessRoleName\n        bufferResourceName\n        bufferResourceArn\n        tags {\n            key\n            value\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getAppPipeline\": {\n            \"id\": \"67409286-2672-413f-b477-dc1f4d7966d4\",\n            \"bufferType\": \"S3\",\n            \"bufferParams\": [\n                {\n                    \"paramKey\": \"logBucketName\",\n                    \"paramValue\": \"log-bucket\"\n                },\n                {\n                    \"paramKey\": \"logBucketPrefix\",\n                    \"paramValue\": \"AppLogs/my-index/year=%Y/month=%m/day=%d\"\n                },\n                {\n                    \"paramKey\": \"defaultCmkArn\",\n                    \"paramValue\": \"arn:aws:kms:eu-west-1:123456789012:key/9619ed02-b533-4d49-91de-3dd8efa11135\"\n                },\n                {\n                    \"paramKey\": \"maxFileSize\",\n                    \"paramValue\": \"50\"\n                },\n                {\n                    \"paramKey\": \"uploadTimeout\",\n                    \"paramValue\": \"60\"\n                },\n                {\n                    \"paramKey\": \"compressionType\",\n                    \"paramValue\": \"gzip\"\n                }\n            ],\n            \"aosParams\": {\n                \"opensearchArn\": \"arn:aws:es:eu-west-1:123456789012:domain/dev\",\n                \"domainName\": \"dev\",\n                \"indexPrefix\": \"my-index\",\n                \"warmLogTransition\": 0,\n                \"coldLogTransition\": 0,\n                \"logRetention\": 180,\n                \"shardNumbers\": 5,\n                \"replicaNumbers\": 1,\n                \"engine\": \"OpenSearch\"\n            },\n            \"createdDt\": \"2022-11-30T03:03:56Z\",\n            \"status\": \"ACTIVE\",\n            \"bufferAccessRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-AppPipe-67409-BufferAccessRoleDF53FD85-BTLM263CB8JI\",\n            \"bufferAccessRoleName\": \"LogHub-AppPipe-67409-BufferAccessRoleDF53FD85-BTLM263CB8JI\",\n            \"bufferResourceName\": \"log-bucket\",\n            \"bufferResourceArn\": \"arn:aws:s3:::log-bucket\",\n            \"tags\": [\n                {\n                    \"key\": \"hello\",\n                    \"value\": \"world\"\n                }\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#application-log-ingestion-apis","title":"Application Log Ingestion APIs","text":"<p>The following operations are available in the solution's Application (App) Log Ingestion APIs.</p>"},{"location":"designs/app-log/api-design/#create-app-log-ingestion","title":"Create App Log Ingestion","text":"<p>Type: Mutation</p> <p>Description: Create a record in DynamoDB</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description appPipelineId K-V Yes Selected Amazonn OpenSearch related parameters. confId K-V Yes Created Kinesis Data Stream related parameters. groupIds String[] Yes Created Kinesis Data Stream related parameters. stackId String Yes In the process of creating an application log pipeline, KDS and Lambda are created through the CloudFormation stack. This item can be obtained through the listAppLogIngestions API. stackName String Yes In the process of creating an application log pipeline, KDS and Lambda are created through the CloudFormation stack. This item can be obtained through the listAppLogIngestions API. <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n    createAppLogIngestion(\n          appPipelineId: \"45851795-6401-41f7-8ded-6c6db14f375c\",\n          confId: \"01523e70-b571-4583-8882-56c877ec098c\",\n          groupIds: [\"afa6c23f-765c-4322-bb00-234525a5ff85\"],\n          stackId: \"\",\n          stackName: \"\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"createAppLogIngestion\": \"2de27afe-d568-49cc-b7b5-86b161ce0662\"\n    }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n    \"data\": {\n        \"createAppLogIngestion\": null\n    },\n    \"errors\": [{\n        \"path\": [\n            \"createAppLogIngestion\"\n        ],\n        \"data\": null,\n        \"errorType\": \"Lambda:Unhandled\",\n        \"errorInfo\": null,\n        \"locations\": [{\n            \"line\": 23,\n            \"column\": 3,\n            \"sourceName\": null\n        }],\n        \"message\": \"please check groupId afa6c23f-765c-4322-bb00-234525a5ff85 and conId 01523e70-b571-4583-8882-56c877ec098c, they already exist in applineId 45851795-6401-41f7-8ded-6c6db14f375c\"\n    }]\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#delete-app-log-ingestion","title":"Delete App Log Ingestion","text":"<p>Type: Mutation</p> <p>Description: We don't physically delete the record, we just set the state of the item to INACTIVE in DynamoDB Table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description ids String[] Yes Log Ingestion ID Set (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n        deleteAppLogIngestion(\n              ids: [\"60779959-95e3-45b6-a433-225f5c57edcc\", \"86b02ebc-d952-4b37-ac17-f001150d3a16\"]\n              )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAppLogIngestion\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#list-app-log-ingestions","title":"List App Log Ingestions","text":"<p>Type: Query</p> <p>Description: List all Ingestion</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description appPipelineId String Yes 10 Application Pipeline Unique Id count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n    listAppLogIngestions(appPipelineId: \"45851795-6401-41f7-8ded-6c6db14f375c\", count: 10, page: 1) {\n        appLogIngestions {\n            appPipelineId\n            confId\n            confName\n            createdDt\n            groupId\n            groupName\n            stackName\n            stackId\n            id\n            tags [{\n                key\n                value\n            }]\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listAppLogIngestions\": {\n            \"appLogIngestions\": [{\n                    \"appPipelineId\": \"45851795-6401-41f7-8ded-6c6db14f375c\",\n                    \"confId\": \"01523e70-b571-4583-8882-56c877ec098c\",\n                    \"confName\": \"c2\",\n                    \"createdDt\": \"2021-11-16T11:26:35.509759\",\n                    \"groupId\": \"afa6c23f-765c-4322-bb00-234525a5ff85\",\n                    \"groupName\": \"g4\",\n                    \"stackName\": \"\",\n                    \"stackId\": \"\",\n                    \"id\": \"dd0eb789-6a33-4b51-873d-f5473ccdf144\",\n                    \"tags\": []\n                },\n                {\n                    \"appPipelineId\": \"45851795-6401-41f7-8ded-6c6db14f375c\",\n                    \"confId\": \"01523e70-b571-4583-8882-56c877ec098c\",\n                    \"confName\": \"c2\",\n                    \"createdDt\": \"2021-11-16T11:26:35.509716\",\n                    \"groupId\": \"8ef2debb-1c72-4821-9e61-ce89b6c6ed00\",\n                    \"groupName\": \"g3\",\n                    \"stackName\": \"\",\n                    \"stackId\": \"\",\n                    \"id\": \"af65c64b-7403-4e92-90f6-1ec13d655deb\",\n                    \"tags\": []\n                }\n            ],\n            \"total\": 2\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/api-design/#get-app-log-ingestion-details","title":"Get App Log Ingestion Details","text":"<p>Type: Query</p> <p>Description: Get details of a Ingestion.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes App Log Ingestion Unique ID (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getAppLogIngestion(id: \"5051c5ce-f0fb-4b6e-be39-05490756b335\") {\n        appPipelineId\n        confId\n        createdDt\n        groupId\n        id\n        stackId\n        stackName\n        tags[{\n            key\n            value\n        }]\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getAppLogIngestion\": {\n            \"appPipelineId\": \"f45648b9-cfa8-4bfb-bf6b-f7a06a8fecf1\",\n            \"confId\": \"c1\",\n            \"createdDt\": \"2021-11-07T17:48:03.935902\",\n            \"groupId\": \"g1\",\n            \"id\": \"5051c5ce-f0fb-4b6e-be39-05490756b335\",\n            \"stackId\": \"s\",\n            \"stackName\": \"ss\",\n            \"tags\": []\n        }\n    }\n}\n</code></pre>"},{"location":"designs/app-log/architecture-design/","title":"Application Log Analytics Design","text":""},{"location":"designs/app-log/architecture-design/#overview","title":"Overview","text":"<p>Application Log Analytics, as one module of Log Hub solution, is used to collect logs for Application, process and ingest into Amazon OpenSearch Service (AOS). This document is to describe this module is designed.</p> <p>Currently, this solution supports JSON format, Nginx Format, Apache Format, Spring Boot Logs, Single-line text, Multi-line text.</p> <p>Info</p> <p>For more information about solution overall design, refer to Architecture Design.</p>"},{"location":"designs/app-log/architecture-design/#high-level-design","title":"High Level Design","text":"<p>Model Layer -The proxy of the back-end service encapsulates the model required by the view layer and defines the output content by the caller.</p> <p>Resources - Here we specifically refer to the Amazon Web Services created and invoked through APIs in the solution, such as EC2 instances that need to transmit log data, Systems Manager used by Fluent Bit installed, Amazon Kinesis Data Streams, Amazon Lambda, Amazon OpenSearch Service used for log storage and data analysis and presentation.</p> <p>Log Config Service - To used to describe log configuration information, including log location, log type, search engine field type, etc.</p> <p>Instance Group Service - An instance Group is a collection of instances. Currently, only instances in the same region as Log Hub are supported. This service is responsible for the logical classification of instances, the installation of Fluent Bit on the instance, and the status detection of Fluent Bit. For the installation of Fluent Bit, the system is processed through multi-threading.</p> <p>Application Log Pipeline Service - Responsible for asynchronous creation of data buffers, data buffer automatic scaling service, and log processor instance.</p> <p>Application Log Ingestion Service - Responsible for generating configuration files, distributing configuration through SSM, and scheduling Fluent Bit for data transmission, and creating OpenSearch templates according to the log field data types defined by Log config to ensure that the data written to OpenSearch conforms to the preset data types.</p> <p>Kinesis Data Streams Auto Scaling Service Amazon Kinesis Data Streams is automatically scaled by using Amazon CloudWatch and AWS Lambda if the user turns on autoscaling for the data buffer.</p>"},{"location":"designs/app-log/data-model-design/","title":"Application Log Analytics Data Model Design","text":""},{"location":"designs/app-log/data-model-design/#overview","title":"Overview","text":"<p>This part uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Application Log Analytics module. </p>"},{"location":"designs/app-log/data-model-design/#entity-relationship-diagram","title":"Entity Relationship Diagram","text":""},{"location":"designs/app-log/data-model-design/#table-design","title":"Table Design","text":""},{"location":"designs/app-log/data-model-design/#logconf-table","title":"LogConf Table","text":"<p>LogConf table stores information about the Application log configuration by this solution, such as log type, log path.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a configuration Partition key confName String The name of the configuration name createdDt String creation time logPath String Log file path logType String Json, Regex, Nginx, Apache, MultiLineText multilineLogParser String JAVA_SPRING_BOOT regularExpression String Regular expressions regularSpec String Field type definition after regular parsing status String ACTIVE, INACTIVE INACTIVE means delete state userLogFormat String Log format updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#instancegroup-table","title":"InstanceGroup Table","text":"<p>InstanceGroup table stores information about the instance and grouping relationship information by this solution, such as log type.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a group Partition key groupName String The name of the log group. the name must be unique, and can only contains lower case letters and -. createdDt String Creation time instanceSet String List of instance ids status String ACTIVE, INACTIVE INACTIVE means delete state updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#instancemeta-table","title":"InstanceMeta Table","text":"<p>InstanceMeta table stores information about the instance ingestion by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a configuration Partition key createdDt String Creation time intanceId String The EC2 instance id appPipelineId String The Partition key of the AppPipeline table logAgent Map sub-field: agentName: FluentBit sub-field: version: 1.8.2 confId String The Partition key of the LogConf table groupId String The Partition key of the InstanceGroup table status String ACTIVE, INACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#logagentstatus-table","title":"LogAgentStatus Table","text":"<p>LogAgentStatus table stores information about the status of Fluent Bit installation by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments intanceId String the EC2 instance Id Partition key createdDt String creation time id String the Command Id status String Not_Installed, Online, Offline updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#apppipeline-table","title":"AppPipeline Table","text":"<p>AppPipeline table stores information about Application Log Pipeline by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of the pipeline Partition key aosParams Map OpenSearch details (e.g. OpenSearch domain Arn, endpoint etc.) bufferParams List List of Map (paramKey, paramValue) bufferType String Type of buffer used in pipeline (such as KDS, S3) bufferResourceArn String Buffer Resource Arn (e.g. if buffer is S3, then it's S3 bucket ARN) bufferResourceName String Buffer Resource Name (e.g. if buffer is S3, then it's S3 bucket Name) osHelperFnArn String A helper Function ARN stackId String CloudFormation Stack ID error String CloudFormation Stack Error if any tags List List of Map (Key-Value) status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE createdDt String creation time"},{"location":"designs/app-log/data-model-design/#applogingestion-table","title":"AppLogIngestion Table","text":"<p>AppLogIngestion table is used to the information about Application Log Ingestion by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a pipeline Partition key createdDt String creation time confId String The Partition key of the LogConf table sourceType String EC2,EKS, S3 sourceId String If EC2 then sourceId is groupId; If EKS then sourceId is EKSClusterId; If S3 then sourceId is S3LogSourceInfo; groupId String The Partition key of the InstanceGroup table stackId String The Cloudformation stack ID for ingesting application logs from the S3 bucket or K8s pod. stackName String The Cloudformation stack Name for ingesting application logs from the S3 bucket or K8s pod. appPipelineId String The Partition key of the AppPipeline table tags Map Sub-field: key-value, type:String status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/data-model-design/#eksclusterlogsource-table","title":"EKSClusterLogSource Table","text":"<p>EKSClusterLogSource table stores information about imported EKS Cluster by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Description Comments id String Unique ID of a pipeline Partition key createdDt String creation time aosDomainId String The Partition key of the Cluster table region String The region to which the imported EKS cluster belongs accountId String The account to which the imported EKS cluster belongs eksClusterName String The Partition key of the InstanceGroup table eksClusterArn String The imported EKS Cluster ARN. cri String The K8s Container runtime : containerd,docker. subnetIds String The EKS Cluster Subnets vpcId Map The EKS Cluster vpcId eksClusterSGId String The EKS Cluster security group oidcIssuer String OpenID Connect provider URL endpoint String The EKS Cluster API server endpoint deploymentKind String DaemonSet,Sidecar tags Map Sub-field: key-value, type:String logAgentRoleArn String The ARN of the role corresponding to the service account of K8s, this role attaches write-related permissions to KDS. status String CREATING, DELETING, ERROR, INACTIVE, ACTIVE updatedDt String The last time the data was updated"},{"location":"designs/app-log/log-analytics-pipeline-app/","title":"Application Log","text":"<p>A Log Pipeline includes the process of receiving, cleaning, enhancing, and writing data to AOS. Typically, a log pipeline only accepts logs in one format. A log analytics pipeline corresponds to an index pattern in AOS (e.g. <code>log-hub-nginx-log-index</code>)\u3002</p>"},{"location":"designs/app-log/log-analytics-pipeline-app/#system-architecture-design","title":"System Architecture design","text":""},{"location":"designs/app-log/log-analytics-pipeline-app/#concept","title":"Concept","text":""},{"location":"designs/app-log/log-analytics-pipeline-app/#log-config","title":"Log Config","text":"<p>Log Config contains</p> <ul> <li> <p>Config Name: The name of the log configuration, the name must be unique, and can only contains lower case letters and -.</p> </li> <li> <p>Log Path: Specify the log file locations. If you have multiple locations, please write all the locations and split using ' , '. e.g./var/log/app1/.log,/var/log/app2/.log. All files in the specified folder that match the file name will be monitored. The file name can be a full name or wildcard pattern matching is supported.</p> </li> <li> <p>Log Type: the Log Hub has built-in plugins to parse log data from log agents. Currently supported log types are as follows:</p> </li> <li> <p>JSON, Nginx Log, Apache Log, Spring Boot Log</p> </li> <li> <p>Additionally, we also point to custom parsing via regular expressions</p> </li> <li> <p>Log Format: the log format configuration in Nginx, Spring Boot, Apache configuration files. Such as Nginx, if you want to know more, you can refer to configuring logging in Nginx</p> </li> </ul>"},{"location":"designs/app-log/log-analytics-pipeline-app/#log-group","title":"Log Group","text":"<p>A Log Group is a collection of one or more Log Configs applied to a group of EC2 instances. The following figure can better understand the concepts of Log Group and Log Config. </p> <p>According to the configuration shown in the figure above, the configuration files of FluentBit in Instance A, B, and C are all different.</p> <ul> <li>Instance A: Collect Nginx type log a and JSON format log b.</li> <li>Instance B: Collect all four types of logs.</li> <li>Instance C: Collect Apache type logs c and JSON format logs d.</li> </ul>"},{"location":"designs/app-log/log-analytics-pipeline-app/#faq","title":"FAQ","text":"<p>Q. Why not use Firehose to collect and write to AOS?</p> <p>Because the minimum buffer interval of Firehose is 60s, it is difficult to meet the scene of real-time class analysis. Please refer to Amazon Kinesis Data Firehose Quota.</p>"},{"location":"designs/app-log/process-design/","title":"Application Log Pipeline Process","text":"<p>This document is about the Process Design for Log Agent Installation, Application Log Pipeline and Ingestion.</p>"},{"location":"designs/app-log/process-design/#overview","title":"Overview","text":""},{"location":"designs/app-log/process-design/#install-log-agent","title":"Install Log Agent","text":""},{"location":"designs/app-log/process-design/#create-an-application-log-ingestion","title":"Create an Application Log Ingestion","text":""},{"location":"designs/domain-management/api-design/","title":"Domain Management API Design","text":""},{"location":"designs/domain-management/api-design/#overview","title":"Overview","text":"<p>This document is about the API Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/api-design/#domain-apis","title":"Domain APIs","text":"<p>Domain APIs are a list of operations on top of Amazon OpenSearch Service (AOS). </p> <p>The following operations are available in the solution's Domain APIs.</p>"},{"location":"designs/domain-management/api-design/#list-domain-names","title":"List Domain Names","text":"<p>Type: Query</p> <p>Description:  List all existing Amazon OpenSearch domains in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description region String No current region To support cross region listing (in the same account) <p>Simple Request &amp; Response:</p> <p>Request with region </p> <pre><code>query example{\n  listDomainNames(region: \"us-west-2\") {\n    domainNames\n  }\n}\n</code></pre> <p>Request without region</p> <pre><code>query example {\n  listDomainNames {\n    domainNames\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listDomainNames\": {\n      \"domainNames\": [\n        \"dev\",\n        \"test\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#import-domain","title":"Import Domain","text":"<p>Type: Mutation</p> <p>Description:  Import an Exisiting Amazon OpenSearch Domain,  store general info from DynamoDB table.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description domainName String Yes Amazon OpenSearch Domain Name region String No current region To support cross region Amazon OpenSearch import vpc K-V Yes Log processing vpc tags K-V No Custom tags for the imported domain <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example{\n  importDomain(\n    domainName: \"dev\", \n    tags: {key: \"project\", value: \"Loghub\"},\n    vpc: {\n        securityGroupId: \"sg-1\", \n        vpcId: \"vpc-1\", \n        privateSubnetIds: \"subnet-a,subnet-b\", \n        publicSubnetIds: \"subnet-c,subnet-d\"\n    },\n    region: \"us-west-2\"\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"importDomain\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Domain is already imported</li> <li>Elasticsearch Domain Not Found</li> <li>Public network type is not supported, only Amazon OpenSearch domain within VPC can be imported</li> <li>The domain to be imported must be active</li> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"importDomain\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"importDomain\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 8,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Domain is already imported\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#remove-domain","title":"Remove Domain","text":"<p>Type: Mutation</p> <p>Description:  Remove an Amazon OpenSearch Domain record from DynamoDB table. This will not remove the backend AOS domain.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  removeDomain(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"removeDomain\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul> <pre><code>{\n  \"data\": {\n    \"removeDomain\": null\n  },\n  \"errors\": [\n    {\n      \"path\": [\n        \"removeDomain\"\n      ],\n      \"data\": null,\n      \"errorType\": \"Lambda:Unhandled\",\n      \"errorInfo\": null,\n      \"locations\": [\n        {\n          \"line\": 32,\n          \"column\": 3,\n          \"sourceName\": null\n        }\n      ],\n      \"message\": \"Unknown exception, please check Lambda log for more details\"\n    }\n  ]\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#list-imported-domains","title":"List Imported Domains","text":"<p>Type: Query</p> <p>Description:  List all existing Amazon OpenSearch domains in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description metrics Boolean No FALSE To decide wheather need to query domain metrix (additional request) <p>Simple Request &amp; Response:</p> <p>Request: </p> <pre><code>query example {\n  listImportedDomains(metrics: true) {\n    domainName\n    endpoint\n    id\n    metrics {\n      freeStorageSpace\n      health\n      searchableDocs\n    }\n    version\n    engine\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listImportedDomains\": [\n      {\n        \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n        \"domainName\": \"dev\",\n        \"endpoint\": \"vpc-dev-3ze2yoxxxxxxxxx.us-west-2.es.amazonaws.com\",\n        \"metrics\": {\n          \"freeStorageSpace\": 16058.91,\n          \"health\": \"GREEN\",\n          \"searchableDocs\": 13159\n        },\n        \"version\": \"1.0\",\n        \"engine\": \"OpenSearch\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#get-domain-details","title":"Get Domain Details","text":"<p>Type: Query</p> <p>Description:  Get details of an imported domain.</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) metrics Boolean No FALSE Whether to include metrics <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n  getDomainDetails(id: \"439239da8014f9a419c92b1b0c72a5fc\") {\n    domainName\n    endpoint\n    id\n    nodes {\n      coldEnabled\n      dedicatedMasterCount\n      dedicatedMasterEnabled\n      dedicatedMasterType\n      instanceCount\n      instanceType\n      warmCount\n      warmEnabled\n      warmType\n      zoneAwarenessEnabled\n    }\n    tags {\n      key\n      value\n    }\n    storageType\n    volume {\n      size\n      type\n    }\n    vpc {\n      privateSubnetIds\n      publicSubnetIds\n      securityGroupId\n      vpcId\n    }\n    metrics {\n      freeStorageSpace\n      health\n      searchableDocs\n    }\n    engine\n    version\n    proxyALB\n    proxyError\n    proxyInput {\n      certificateArn\n      cognitoEndpoint\n      customEndpoint\n      keyName\n      vpc {\n        privateSubnetIds\n        publicSubnetIds\n        securityGroupId\n        vpcId\n      }\n    }\n    proxyStatus\n    alarmError\n    alarmInput {\n      email\n      phone\n      alarms {\n        type\n        value\n      }\n    }\n    alarmStatus\n    cognito {\n      domain\n      enabled\n      identityPoolId\n      userPoolId\n      roleArn\n    }\n    accountId\n    domainArn\n    region\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getDomainDetails\": {\n      \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n      \"domainName\": \"dev\",\n      \"endpoint\": \"vpc-dev-3ze2yoxxxxxxxxx.us-west-2.es.amazonaws.com\",\n      \"engine\": \"OpenSearch\",\n      \"version\": \"1.0\"\n      \"vpc\": {\n        \"privateSubnetIds\": \"subnet-1234\",\n        \"publicSubnetIds\": \"subnet-6789\",\n        \"securityGroupId\": \"sg-1\",\n        \"vpcId\": \"vpc-1\"\n      },\n      \"cognito\": {\n        \"domain\": \"\",\n        \"enabled\": false,\n        \"identityPoolId\": \"N/A\",\n        \"userPoolId\": \"N/A\",\n        \"roleArn\": \"N/A\"\n      }\n      \"nodes\": {\n        \"coldEnabled\": false,\n        \"dedicatedMasterCount\": 0,\n        \"dedicatedMasterEnabled\": false,\n        \"dedicatedMasterType\": \"N/A\",\n        \"instanceCount\": 1,\n        \"instanceType\": \"r6g.large.elasticsearch\",\n        \"warmCount\": 0,\n        \"warmEnabled\": false,\n        \"warmType\": \"N/A\",\n        \"zoneAwarenessEnabled\": false\n      },\n      \"tags\": [\n        {\n          \"key\": \"project\",\n          \"value\": \"Loghub\"\n        }\n      ],\n      \"storageType\": \"EBS\",\n      \"volume\": {\n        \"size\": 100,\n        \"type\": \"gp2\"\n      }\n      \"esVpc\": {\n        \"availabilityZones\": [\n          \"us-west-2b\"\n        ],\n        \"securityGroupIds\": [\n          \"sg-07cdfb011fba47e27\"\n        ],\n        \"subnetIds\": [\n          \"subnet-0f88a069\"\n        ],\n        \"vpcId\": \"vpc-538e702a\"\n      },  \n      \"metrics\": {\n        \"freeStorageSpace\": 1,\n        \"health\": \"GREEN\",\n        \"searchableDocs\": 1\n      },\n      \"alarmError\": \"\",\n      \"alarmInput\": {\n        \"email\": \"test@example.com\",\n        \"phone\": null,\n        \"alarms\": [\n          {\n            \"type\": \"CLUSTER_RED\",\n            \"value\": \"true\"\n          }\n        ]\n      },\n      \"alarmStatus\": \"ENABLED\",\n      \"proxyStatus\": \"ENABLED\",\n      \"proxyALB\": \"LogHu-LoadB-xxx.us-west-2.elb.amazonaws.com\",\n      \"proxyError\": \"\"\n      \"proxyInput\": {\n        \"certificateArn\": \"arn:aws:es:us-west-2:123456789012:domain/mycert\",\n        \"cognitoEndpoint\": \"\",\n        \"customEndpoint\": \"www.example.com\",\n        \"keyName\": \"my-key\",\n        \"vpc\": {\n          \"publicSubnetIds\": \"subnet-1234,subnet-1235\",\n          \"privateSubnetIds\": \"subnet-5678,subnet-5679\",\n          \"securityGroupId\": \"sg-1234\",\n          \"vpcId\": \"vpc-1234\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Cannot find domain in the imported list</li> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#get-domain-vpc","title":"Get Domain VPC","text":"<p>Type: Query</p> <p>Description:  Get VPC info of an Amazon OpenSearch domain in a region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description region String No current region To support cross region listing (in the same account) domainName String Yes Domain Name <p>Simple Request &amp; Response:</p> <p>Request with region </p> <pre><code>query example {\n  getDomainVpc(domainName: \"dev\", region: \"eu-west-1\") {\n    availabilityZones\n    securityGroupIds\n    subnetIds\n    vpcId\n  }\n}\n</code></pre> <p>Request without region</p> <pre><code>query example {\n  getDomainVpc(domainName: \"dev\") {\n    availabilityZones\n    securityGroupIds\n    subnetIds\n    vpcId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getDomainVpc\": {\n      \"availabilityZones\": [\n        \"eu-west-1a\"\n      ],\n      \"securityGroupIds\": [\n        \"sg-07cdfb011fba47e27\"\n      ],\n      \"subnetIds\": [\n        \"subnet-0f88a069\"\n      ],\n      \"vpcId\": \"vpc-538e702a\"\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#create-proxy-for-opensearch","title":"Create Proxy For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Create a Nginx Proxy for Amazon OpenSearch in vpc</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description customEndpoint String Yes Custom Domain to access Kibana cognitoEndpoint String No Cognito Domain for Amazon OpenSearch, blank if Amazon OpenSearch doesn't have cognito enabled id String Yes Amazon OpenSearch Domain Arn keyName String Yes? EC2 (nginx) key name vpc K-V Yes VPC for EC2 (nginx) certificateArn String Yes ACM certificate Arn for ELB <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  createProxyForOpenSearch(\n    nginx: {\n        vpc: {\n            securityGroupId: \"sg-1234\", \n            privateSubnetIds: \"subnet-1234,subnet-1235\",\n            publicSubnetIds: \"subnet-5678,subnet-5679\", \n            vpcId: \"vpc-1234\"\n        }, \n        certificateArn: \"arn:aws:es:us-west-2:123456789012:domain/mycert\", \n        keyName: \"my-key\",\n        cognitoEndpoint: \"hello.auth.us-west-2.amazoncognito.com\", \n        customEndpoint: \"www.example.com\",\n    }, \n\n    id: \"439239da8014f9a419c92b1b0c72a5fc\"\n  )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createNginxProxyForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#delete-proxy-for-opensearch","title":"Delete Proxy For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Remove an Amazon OpenSearch Nginx Proxy Stack</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Request:</p> <pre><code>mutation example {\n  deleteProxyForOpenSearch(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteProxyForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#create-alarm-for-opensearch","title":"Create Alarm For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Create an Alarm for opensearch domain</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn email String Yes? Email to receive notification alarms List Yes List of k-v for alarm parameters phone String No Phone number to receive notification <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  createAlarmForOpenSearch(\n    id: \"439239da8014f9a419c92b1b0c72a5fc\", \n    input: {\n        email: \"test@example.com\", \n        alarms: [\n            {Type: CLUSTER_RED, value: \"true\"},\n            {Type: FREE_STORAGE_SPACE, value: \"20\"}\n            ...\n    })\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createAlarmForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#delete-alarm-for-opensearch","title":"Delete Alarm For OpenSearch","text":"<p>Type: Mutation</p> <p>Description:  Remove an Alarm Stack</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Amazon OpenSearch Domain Arn (key in DynamoDB) <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  deleteAlarmForOpenSearch(id: \"439239da8014f9a419c92b1b0c72a5fc\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteAlarmForOpenSearch\": \"OK\"\n  }\n}\n</code></pre> <p>Exceptions:</p> <ul> <li>Unknown exception, please check Lambda log for more details</li> </ul>"},{"location":"designs/domain-management/api-design/#resource-apis","title":"Resource APIs","text":"<p>Resource APIs are a list of helper functions for AWS Resources that are used in the solution, such as listing VPCs etc.</p>"},{"location":"designs/domain-management/api-design/#list-resources","title":"List Resources","text":"<p>Type: Query</p> <p>Description:  List AWS Resources (Services) in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, VPC, Subnet, SecurityGroup, Certificate, KeyName, Trail parentId String No To filter by parent Id if any, if not provided, all are returned <p>Simple Request &amp; Response:</p> <p>Request for list S3Bucket</p> <pre><code>query example {\n  listResources(Type:S3Bucket) {\n    id\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"bucketa\",\n        \"name\": \"bucketa\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"bucketb\",\n        \"name\": \"bucketb\",\n        \"parentId\": null\n      }\n    ]\n  }\n}\n</code></pre> <p>Request for list VPC Id</p> <pre><code>query example {\n  listResources(Type:VPC) {\n    id\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"vpc-040e5096a29a457db\",\n        \"name\": \"test-vpc\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"vpc-538e702a\",\n        \"name\": \"default-vpc\",\n        \"parentId\": null\n      },\n      {\n        \"id\": \"vpc-1112456\",\n        \"name\": \"-\",\n        \"parentId\": null\n      }\n    ]\n  }\n}\n</code></pre> <p>Request for list Subnet Id</p> <pre><code>query example {\n  listResources(Type:Subnet, parentId: \"vpc-088c09a3e0b797406\") {\n    id\n    description\n    name\n    parentId\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"id\": \"subnet-00a5510951c6b4bad\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/publicSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-066f81646e30f0e48\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/publicSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-06c232cfb88789980\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/privateSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-04324df4484d33cfb\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/privateSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-065c2b45471b08568\",\n        \"description\": \"eu-west-1b\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/isolatedSubnet2\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      },\n      {\n        \"id\": \"subnet-0934357dfce96eba3\",\n        \"description\": \"eu-west-1a\",\n        \"name\": \"LogHub/LogHubVPC/DefaultVPC/isolatedSubnet1\",\n        \"parentId\": \"vpc-088c09a3e0b797406\"\n      }\n    ]\n  }\n} \n</code></pre> <p>Request for list lambda functions</p> <pre><code>query example {\n  listResources(Type:Lambda) {\n    description\n    id\n    name\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"description\": \"Log Hub - Helper function to handle CloudFormation deployment\",\n        \"id\": \"LogHub-LogHubCfnFlowCfnHelperD9302B91-VZNJXLqIZjVu\",\n        \"name\": \"LogHub-LogHubCfnFlowCfnHelperD9302B91-VZNJXLqIZjVu-$LATEST\"\n      },\n      ...\n    ]\n  }\n}\n</code></pre> <p>Request for list RDS instances</p> <pre><code>query example {\n  listResources(Type:RDS) {\n    description\n    id\n    name\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listResources\": [\n      {\n        \"description\": \"/aws/rds/instance/database-1\",\n        \"id\": \"database-1\",\n        \"name\": \"database-1 (mysql)\"\n      },\n      {\n        \"description\": \"/aws/rds/cluster/demodb\",\n        \"id\": \"demodb-instance-1\",\n        \"name\": \"demodb-instance-1 (aurora-mysql)\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/api-design/#put-resource-logging-bucket","title":"Put Resource Logging Bucket","text":"<p>Type: Mutation</p> <p>Description:  Put Logging bucket for resource in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, CloudFront resourceName String Yes The resource name or ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  putResourceLoggingBucket(resourceName: \"test-bucket\", Type: S3Bucket) {\n    bucket\n    prefix\n    enabled\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"putResourceLoggingBucket\": {\n      \"bucket\": \"loghub-loghubloggingbucket0fa53b76-mkvj68ix2ufo\",\n      \"prefix\": \"s3/test-bucket/\",\n      \"enabled\": true\n    }\n  }\n} \n</code></pre>"},{"location":"designs/domain-management/api-design/#get-resource-logging-bucket","title":"Get Resource Logging Bucket","text":"<p>Type: Query</p> <p>Description:  Get Logging bucket for resource in current region</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Available List: S3Bucket, Trail resourceName String Yes The resource name or ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  getResourceLoggingBucket(Type: Trail, resourceName: \"testtrail\") {\n    bucket\n    prefix\n    enabled\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getResourceLoggingBucket\": {\n      \"bucket\": \"aws-cloudtrail-logs-123456789012-222dcf7b\",\n      \"prefix\": \"AWSLogs/123456789012/CloudTrail/\",\n      \"enabled: true\n    }\n  }\n}\n</code></pre>"},{"location":"designs/domain-management/component-design/","title":"Domain Management Component Design","text":""},{"location":"designs/domain-management/component-design/#overview","title":"Overview","text":"<p>Log Hub solution uses Amazon OpenSearch service (AOS) as the underlying engine to store and analyze logs. This component consists a list of operations on top of AOS domains.</p> <p>This document is to describe this component is designed.</p> <p>Info</p> <p>For more information about solution overall design, refer to High Level Design.</p>"},{"location":"designs/domain-management/component-design/#component-design","title":"Component Design","text":""},{"location":"designs/domain-management/component-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>This component contains two sub components.</p> <ul> <li> <p>Proxy for AOS</p> <p>As OpenSearch Dashboards is within VPC and has no public accesses. Customer can choose to deploy a proxy stack to access the OpenSearch Dashboards from internet with a custom domain.</p> <p>Below is the high level architecture diagram:</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>Customer accesses custom domain for the proxy, the domain needs to be resolved via DNS service (for example, using Route 53 on AWS)</p> </li> <li> <p>The DNS service routes the traffic to internet-facing Application Load Balancer (ALB)</p> </li> <li> <p>The ALB distributes web traffic to backend Nginx server running on Amazon EC2 within Auto Scaling Group. </p> </li> <li> <p>The Nginx server redirects the requests to OpenSearch Dashboards.</p> </li> <li> <p>(optional) VPC peering is required if the VPC for the proxy is not the same one as the OpenSearch service.</p> </li> </ol> <p>Info</p> <p>This stack can be deployed independently without the UI, check more details about the CloudFormation Design</p> </li> <li> <p>Alarm for AOS</p> <p>There are a list of recommended CloudWatch alarms to be set up for Amazon OpenSearch Service. For example, to sent an email if the cluster health status is red for longer than one minute. Customer can choose to deploy an Alarm stack with one click to set up alarms in AWS.</p> <p>Below is the high level architecture diagram:</p> <p></p> <p>Info</p> <p>This stack can be deployed indepentdantly without the UI, check more details about the CloudFormation Design</p> <p>The process is described as below:</p> <ol> <li> <p>CloudWatch Alarm to monitor Amazon OpenSearch service and send state change event to Amazon EventBridge</p> </li> <li> <p>Amazon EventBridge rule to trigger and send information to Amazon SNS as target</p> </li> <li> <p>Amazon SNS uses Email as subscription and notifies Administrators</p> </li> </ol> </li> </ul>"},{"location":"designs/domain-management/component-design/#process-design","title":"Process Design","text":"<p>This components includes a list of processes for domain management.</p> <p>For details about how the processes are designed, please refer to Process Design</p>"},{"location":"designs/domain-management/component-design/#api-design","title":"API Design","text":"<p>This solution uses GraphQL APIs built on AWS Appsync service.</p> <p>For details about how the backend APIs are designed, please refer to API Design</p>"},{"location":"designs/domain-management/component-design/#data-model-design","title":"Data Model Design","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database to store the information about AOS domains.</p> <p>To learn more information about how the data model is designed, please refer to Data Model Design</p>"},{"location":"designs/domain-management/component-design/#cloudformation-design","title":"CloudFormation Design","text":"<ul> <li>Proxy for AOS CloudFormation Design</li> </ul> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description VPCId <code>&lt;Requires input&gt;</code> The VPC to deploy the Nginx proxy resources, for example, <code>vpc-bef13dc7</code>. PublicSubnetIds <code>&lt;Requires input&gt;</code> The public subnets where ELB are deployed. You need to select at least two public subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. ELBSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group being associated with the ELB, for example, <code>sg-123456</code>. ELBDomain <code>&lt;Requires input&gt;</code> The custom domain name of the ELB, for example, <code>dashboard.example.com</code>. ELBDomainCertificateArn <code>&lt;Requires input&gt;</code> The SSL certificate ARN associated with the ELBDomain. The certificate must be created from [Amazon Certificate Manager (ACM)][acm]. PrivateSubnetIds <code>&lt;Requires input&gt;</code> The private subnets where Nginx instances are deployed. You need to select at least two private subnets, for example, <code>subnet-12345abc, subnet-54321cba</code>. NginxSecurityGroupId <code>&lt;Requires input&gt;</code> The Security group associated with the Nginx instances. The security group must allow access from ELB security group. KeyName <code>&lt;Requires input&gt;</code> The PEM key name of the Nginx instances. EngineType OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. CognitoEndpoint <code>&lt;Optional&gt;</code> The Cognito User Pool endpoint URL of the OpenSearch domain, for example, <code>mydomain.auth.us-east-1.amazoncognito.com</code>. Leave empty if your OpenSearch domain is not authenticated through Cognito User Pool. <ul> <li>Alarm for AOS CloudFormation Design</li> </ul> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description Endpoint <code>&lt;Requires input&gt;</code> The endpoint of the OpenSearch domain, for example, <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code>. DomainName <code>&lt;Requires input&gt;</code> The name of the OpenSearch domain. Email <code>&lt;Requires input&gt;</code> The notification email address. Alarms will be sent to this email address via SNS. ClusterStatusRed <code>Yes</code> Whether to enable alarm when at least one primary shard and its replicas are not allocated to a node. ClusterStatusYellow <code>Yes</code> Whether to enable alarm when at least one replica shard is not allocated to a node. FreeStorageSpace <code>10</code> Whether to enable alarm when a node in your cluster is down to the free storage space you typed in GiB. We recommend setting it to 25% of the storage space for each node. <code>0</code> means the alarm is disabled. ClusterIndexWritesBlocked <code>1</code> Index writes blocked error occurs for &gt;= x times in 5 minutes, 1 consecutive time. Input <code>0</code> to disable this alarm. UnreachableNodeNumber <code>3</code> Nodes minimum is &lt; x for 1 day, 1 consecutive time. <code>0</code> means the alarm is disabled. AutomatedSnapshotFailure <code>Yes</code> Whether to enable alarm when automated snapshot failed. AutomatedSnapshotFailure maximum is &gt;= 1 for 1 minute, 1 consecutive time. CPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred. CPUUtilization or WarmCPUUtilization maximum is &gt;= 80% for 15 minutes, 3 consecutive times. JVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred. JVMMemoryPressure or WarmJVMMemoryPressure maximum is &gt;= 80% for 5 minutes, 3 consecutive times. MasterCPUUtilization <code>Yes</code> Whether to enable alarm when sustained high usage of CPU occurred in master nodes. MasterCPUUtilization maximum is &gt;= 50% for 15 minutes, 3 consecutive times. MasterJVMMemoryPressure <code>Yes</code> Whether to enable alarm when JVM RAM usage peak occurred in master nodes. MasterJVMMemoryPressure maximum is &gt;= 80% for 15 minutes, 1 consecutive time. KMSKeyError <code>Yes</code> Whether to enable alarm when KMS encryption key is disabled. KMSKeyError is &gt;= 1 for 1 minute, 1 consecutive time. KMSKeyInaccessible <code>Yes</code> Whether to enable alarm when KMS encryption key has been deleted or has revoked its grants to OpenSearch Service. KMSKeyInaccessible is &gt;= 1 for 1 minute, 1 consecutive time."},{"location":"designs/domain-management/data-model-design/","title":"Domain Management Data Model Design","text":""},{"location":"designs/domain-management/data-model-design/#overview","title":"Overview","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/data-model-design/#cluster-table","title":"Cluster Table","text":"<p>Cluster table is used to store basic information on imported AOS domain.</p> <p>The data attributes are listed as below:</p> Attribute name Type Example Description Comments id String 439239da8014f9a419c92b1b0c72a5fc MD5 of OpenSearch domain ARN Partition key version String 1.0 OpenSearch Version engine String OpenSearch Either OpenSearch or Elasticsearch region String us-east-1 AWS region endpoint String vpc-dev-i5jwvhie5lzhsfvnxapny.us-east-1.es.amazonaws.com OpenSearch Endpoint domainArn String arn:aws:es:us-east-1:123456789012:domain/dev OpenSearch domain ARN domainName String dev OpenSearch domain name importedDt String 2021-12-20T05:37:20.523951 Date of import proxyStatus String ENABLED OpenSearch Proxy stack status proxyALB String LogHu-LoadB-1T8YLOO675OCN-1782845820.us-east-1.elb.amazonaws.com ELB url for OpenSearch Proxy stack proxyStackId String arn:aws:cloudformation:us-east-1:123456789012:stack/LogHub-Proxy-14682/1b492000-615e-11ec-b5e4-1213fdb3e837 OpenSearch Proxy stack ID proxyInput Map {...} Parameters used when deploy a proxy stack for OpenSearch proxyError String Error messages when deploy a proxy stack for OpenSearch alarmStatus String DISABLED OpenSearch Alarm stack status vpc Map {...} Processing layer VPC when importing domain tags List [{...}] Custom Tags"},{"location":"designs/domain-management/eks-auto-import/","title":"Automatically Import AOS Domain Process","text":""},{"location":"designs/domain-management/eks-auto-import/#overview","title":"Overview","text":"<p>This document is about the process design for Automatically Import AOS Domain method.</p> <p></p>"},{"location":"designs/domain-management/process-design/","title":"Domain Management Process Design","text":""},{"location":"designs/domain-management/process-design/#overview","title":"Overview","text":"<p>This document is about the Process Design for Domain Management component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/domain-management/process-design/#domain-management-process","title":"Domain Management Process","text":""},{"location":"designs/domain-management/process-design/#import-domain","title":"Import Domain","text":"<p>This operation is to import an existing AOS domain into Log Hub solution to ingest logs to.</p> <p>The process to import domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Store AOS information in DynamoDB.</p> <p>When a domain is imported, basic domain information is stored in Cluster table in DynamoDB. </p> <p>Consider only information that can not/might not be changed, such as endpoint, vpc etc. Other information such as EBS volume, Node information can be resized hence is derived via OpenSearch SDK on demand.</p> <p>Solution specified information such as processing layer vpc, tags is also stored in Cluster table.</p> <p>The domain metrics such as free storage are also not stored in Cluster table, as they are changing all the time.</p> </li> <li> <p>Domain ID design</p> <p>Considering that we will support cross account and cross region log ingestion in future, the unique domain ID (partition key in DynamoDB table) must support this.</p> <p>In this solution, we use MD5 of the domain ARN as the domain ID (Assumption is that it's unlikely that two different domain ARNs can have the same MD5 string)</p> </li> <li> <p>Exception handling</p> <p>When a domain is deleting or creating, or a domain is with public network, the import must fail with expection.</p> <p>When importing a domain that is already imported, the import should fail. To avoid such case happening, from frontend, when customer choose from a drop down list of OpenSearch domains, imported domains are excluded from the list.</p> <p>When a domain is already imported and then deleted. the list should not fail.</p> </li> </ol> <p>References:</p> <ul> <li>Import Domain API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#remove-domain","title":"Remove Domain","text":"<p>This operation is to remove an imported AOS domain from Log Hub rather than deleting the AOS domain.</p> <p>The process to remove domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Delete process</p> <p>Deleting an imported domain only removes the item from Cluster table in DynamoDB. The backend OpenSearch domain will not be affected.</p> <p>Also deleting domain will not impact any existing log ingestion pipelines.</p> <p>There is no need to implement soft delete for this process. Customer can easily re-import the domain as needed.</p> </li> </ol> <p>References:</p> <ul> <li>Remove Domain API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#list-imported-domains","title":"List Imported Domains","text":"<p>This operation is to support listing of all the imported domain along with key metrics (Cluster Health, Free Storage Space, Searchable documents).</p> <p>The process to import domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Where to get AOS metrics</p> <p>There are two ways of getting AOS domain metrics such as free storage space, domain health etc. One is to use AOS REST API such as <code>GET _cluster/health</code>, the other is to query in CloudWatch metrics.</p> <p>In this design, CloudWatch metrics are chosen to get domain metrics for two reasons:</p> <p>a) The API backend Lambda doesn't need to have VPC access.</p> <p>b) The CloudWatch metric data is same as what is shown in the AWS Management console.</p> </li> <li> <p>Not all calls need metrics</p> <p>Getting metrics from CloudWatch takes time and bring extra costs. But not all the listing scenerios requires metrics, For example, when customer is choosing an imported domain as a destionation from a list.</p> <p>So an option to choose whether to include metrics is provided. </p> </li> </ol> <p>References:</p> <ul> <li>List Imported Domains API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#get-domain-details","title":"Get Domain Details","text":"<p>This operation is to provide more details about an imported AOS domain, including nodes, volumes, networks etc.</p> <p>The process to Get domain details is described in below diagram:</p> <p></p> <p>Design consideration:</p> <p>Same as List Imported Domain</p> <p>References:</p> <ul> <li>Get Domain Details API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#proxy-for-aos","title":"Proxy for AOS","text":"<p>This operation is to provide a proxy to access OpenSearch dashboards which is within VPC.</p> <p>The process to Create/Delete Proxy for AOS Domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Step Functions Design</p> <p>Use an independent CloudFormation stack to provision resources (such as EC2, ELB etc.) for proxy stacks. Create a reusable Child step function flow for orchestrating the deployment/delete of the sub-stack. When create/delete proxy is triggerred, a parent flow will trigger the child flow to run, and once child flow is completed, the parent flow will be informed will the result and update the status in Cluster table.</p> </li> <li> <p>Store Proxy information in DynamoDB.</p> <p>Store the related parameter key-values of proxy stack in cluster table in DynamoDB. When the proxy is created, the stack id is stored in the table as the stack id is required in order to delete the proxy stack.</p> </li> </ol> <p>References:</p> <ul> <li>Create Proxy for AOS API</li> <li>Delete Proxy for AOS API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/domain-management/process-design/#alarm-for-aos","title":"Alarm for AOS","text":"<p>This operation is to quicly create recommended CloudAlarms to monitor AOS. An email notification will be triggered for alarm.</p> <p>The process to Create/Delete Alarm for AOS Domain is described in below diagram:</p> <p></p> <p>Design consideration:</p> <ol> <li> <p>Step Functions Design</p> <p>Use an independent CloudFormation stack to provision resources (such as CloudWatch alarms, SNS topic etc.) for alarm stacks. Create a reusable Child step function flow for orchestrating the deployment/delete of the sub-stack. When create/delete alarm is triggerred, a parent flow will trigger the child flow to run, and once child flow is completed, the parent flow will be informed will the result and update the status in Cluster table.</p> </li> <li> <p>Store Proxy information in DynamoDB.</p> <p>Store the related parameter key-values of proxy stack in cluster table in DynamoDB. When the alarm is created, the stack id is stored in the table as the stack id is required in order to delete the alarm stack.</p> </li> </ol> <p>References:</p> <ul> <li>Create Alarm for AOS API</li> <li>Delete Alarm for AOS API</li> <li>Cluster Table</li> </ul>"},{"location":"designs/eks-log/api-design/","title":"EKS Log API Design","text":""},{"location":"designs/eks-log/api-design/#eks-log-source-apis","title":"EKS Log Source APIs","text":"<p>This document is about API design of EKS cluster as log source component.</p>"},{"location":"designs/eks-log/api-design/#list-eks-cluster-names","title":"list EKS cluster names","text":"<p>Type: Query</p> <p>Description: Display cluster names for all regions</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description nextToken String No The token for pagination isListAll Boolean No false Whether to show EKS clusters in all regions <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n  listEKSClusterNames(nextToken: \"\", isListAll: false) {\n    clusters\n    nextToken\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listEKSClusterNames\": {\n      \"clusters\": [\n        \"eks-demo\",\n        \"loghub\"\n      ],\n      \"nextToken\": null\n    }\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#import-an-eks-cluster","title":"Import an EKS cluster","text":"<p>Type: Mutation</p> <p>Description: Import an EKS cluster used as log source in solution</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description aosDomainId String Yes The imported AOS domain id. deploymentKind String Yes The deployment type fo the log agent, DaemonSet or SideCar eksClusterName enum Yes The imported EKS cluster name. accountId enum No The account id corresponding to the imported EKS cluster. cri String No K8s container runtime. region String No The region name corresponding to the imported EKS cluster. tags K-V No Tag the EKS Cluster log source. <p>Simple Request &amp; Response:</p> <pre><code>query example{\n  importEKSCluster(\n      aosDomainId: \"7d43abe07ebb4c90af0d8619328054\",\n      deploymentKind: \"DaemonSet\",\n      eksClusterName: \"eks-demo-cluster\",\n      accountId: \"20599832\",\n      cri: \"containerd\",\n      region: \"us-west-2\",\n      tags: {\n          key: \"evn\",\n          value: \"Testing\"\n        }\n    )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"importEKSCluster\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#list-imported-eks-clusters","title":"list imported EKS clusters","text":"<p>Type: Query</p> <p>Description: List imported EKS cluster</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description count Int No 10 page number, start from 1 page Int No 1 number of records per page <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    listImportedEKSClusters (page: 1, count: 10) {\n        eksClusterLogSourceList {\n            id\n            aosDomain {\n                id\n                domainName\n                engine\n                version\n                endpoint\n                metrics {\n                    searchableDocs\n                    freeStorageSpace\n                    health\n                }\n            }\n            eksClusterName\n            eksClusterArn\n            cri\n            vpcId\n            eksClusterSGId\n            subnetIds\n            oidcIssuer\n            endpoint\n            createdDt\n            accountId\n            region\n            logAgentRoleArn\n            deploymentKind\n            tags {\n                key\n                value\n            }\n        }\n        total\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"listImportedEKSClusters\": {\n            \"eksClusterLogSourceList\": [\n                {\n                    \"id\": \"e83aa65ef40e4a8b883dff33af483e36\",\n                    \"aosDomain\": {\n                        \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n                        \"domainName\": \"dev\",\n                        \"engine\": \"OpenSearch\",\n                        \"version\": \"1.0\",\n                        \"endpoint\": \"vpc-dev-xxx.eu-west-1.es.amazonaws.com\",\n                        \"metrics\": null\n                    },\n                    \"eksClusterName\": \"test\",\n                    \"eksClusterArn\": \"arn:aws:eks:eu-west-1:123456789012:cluster/test\",\n                    \"cri\": \"docker\",\n                    \"vpcId\": \"vpc-0123\",\n                    \"eksClusterSGId\": \"sg-1234\",\n                    \"subnetIds\": [\n                        \"subnet-1234\",\n                        \"subnet-5678\",\n                        ...\n                    ],\n                    \"oidcIssuer\": \"https://oidc.eks.eu-west-1.amazonaws.com/id/ABC\",\n                    \"endpoint\": \"https://ABC.sk1.eu-west-1.eks.amazonaws.com\",\n                    \"createdDt\": \"2022-10-29T07:52:48Z\",\n                    \"accountId\": \"123456789012\",\n                    \"region\": \"eu-west-1\",\n                    \"logAgentRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-EKS-LogAgent-Role-3623ec2044264a2189416bcaaa7ee948\",\n                    \"deploymentKind\": \"DaemonSet\",\n                    \"tags\": []\n                },\n                ...\n            ],\n            \"total\": 2\n        }\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#get-imported-eks-cluster-details","title":"Get imported EKS cluster details","text":"<p>Type: Query</p> <p>Description: Display details of an imported eks cluster</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example {\n    getEKSClusterDetails (eksClusterId: \"e83aa65ef40e4a8b883dff33af483e36\") {\n        id\n        aosDomain {\n            id\n            domainName\n            engine\n            version\n            endpoint\n            metrics {\n                searchableDocs\n                freeStorageSpace\n                health\n            }\n        }\n        eksClusterName\n        eksClusterArn\n        cri\n        vpcId\n        eksClusterSGId\n        subnetIds\n        oidcIssuer\n        endpoint\n        createdDt\n        accountId\n        region\n        logAgentRoleArn\n        deploymentKind\n        tags {\n            key\n            value\n        }\n    }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getEKSClusterDetails\": {\n            \"id\": \"e83aa65ef40e4a8b883dff33af483e36\",\n            \"aosDomain\": {\n                \"id\": \"439239da8014f9a419c92b1b0c72a5fc\",\n                \"domainName\": \"dev\",\n                \"engine\": \"OpenSearch\",\n                \"version\": \"1.0\",\n                \"endpoint\": \"vpc-dev-xxx.eu-west-1.es.amazonaws.com\",\n                \"metrics\": null\n            },\n            \"eksClusterName\": \"test\",\n            \"eksClusterArn\": \"arn:aws:eks:eu-west-1:123456789012:cluster/test\",\n            \"cri\": \"docker\",\n            \"vpcId\": \"vpc-0123\",\n            \"eksClusterSGId\": \"sg-1234\",\n            \"subnetIds\": [\n                \"subnet-1234\",\n                \"subnet-5678\",\n                ...\n            ],\n            \"oidcIssuer\": \"https://oidc.eks.eu-west-1.amazonaws.com/id/ABC\",\n            \"endpoint\": \"https://ABC.sk1.eu-west-1.eks.amazonaws.com\",\n            \"createdDt\": \"2022-10-29T07:52:48Z\",\n            \"accountId\": \"123456789012\",\n            \"region\": \"eu-west-1\",\n            \"logAgentRoleArn\": \"arn:aws:iam::123456789012:role/LogHub-EKS-LogAgent-Role-3623ec2044264a2189416bcaaa7ee948\",\n            \"deploymentKind\": \"DaemonSet\",\n            \"tags\": []\n        }\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#eks-log-deployment-apis","title":"EKS Log Deployment APIs","text":""},{"location":"designs/eks-log/api-design/#deploy-as-daemonset","title":"Deploy as DaemonSet","text":"<p>Type: Query</p> <p>Description: Display kubernetes deployment details in yaml for DaemonSet</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example  {\n    getEKSDaemonSetConf (eksClusterId: \"a\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"data\": {\n        \"getEKSDaemonSetConf\": \"...\"\n    }\n}\n</code></pre>"},{"location":"designs/eks-log/api-design/#deploy-as-side-car","title":"Deploy as Side Car","text":"<p>Type: Query</p> <p>Description: Display kubernetes deployment details in yaml for Side Car</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description eksClusterId String Yes log source id for the imported EKS Cluster ingestionId String Yes the Id for application log ingestion <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>query example{\n  getEKSDeploymentConfig(\n      eksClusterId: \"1d431ccd7caa4c90af0d86193bf78f9a\",\n      ingestionId: \"2d43abe07caa4c90af0d8619323064\"\n      )\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getEKSDeploymentConfig\": \"...\"\n  }\n}\n</code></pre>"},{"location":"designs/eks-log/architecture-design/","title":"EKS Cluster Log Analytics Design","text":""},{"location":"designs/eks-log/architecture-design/#overview","title":"Overview","text":"<p>Considering that there are many types of logs in EKS, and different types of logs have different collection sources. But in general, it can be divided into three ways:</p> <ul> <li> <p>The first type of logs is Control plane logging, which includes Audit logs, API server logs, interfacing with CloudWatch, and Controller manager logs. Such logs need to be subscribed in the log group (/aws/eks//cluster) , transfer the newly generated log data to the KDS generated by Log Hub EKS Cluster log Ingestion. We are currently working on Control plane logging. <li> <p>The second is the ELB type Ingress access log. If your ELB type is ALB, this part has been implemented in the Service Log component</p> </li> <li> <p>The last one is the application log in the EKS cluster and the system log on the Node (kube-proxy-XXX.log, aws-load-balancer-controller, aws-node-_kube-system_aws-vpc-cni- init-XXX.log, aws-node-_kube-system_aws-node-XXX.log, coredns-XXX.log). Regarding the use of Nginx as an Ingress, the Ingress access log generated is essentially an application log. Regarding such logs, we will collect the logs by deploying Fluent Bit as a log agent and send the logs to the data buffer created by Log Hub."},{"location":"designs/eks-log/architecture-design/#system-architecture","title":"System Architecture","text":""},{"location":"designs/eks-log/architecture-design/#faq","title":"FAQ","text":"<p>Q. What is the difference between the log pipeline of an EKS cluster and the log pipeline of an application?</p> <p>Considering whether the application is deployed in EC2 or EKS Cluster, there is usually no difference in the application log format, but the analysis methods for logs from different sources are still different. When we designed EKS Cluster log analysis, we abstracted the concept of ==log source== in the application log ingestion component. Therefore, EKS Cluster's log pipeline still uses the application log pipeline. When an EKS Cluster log pipeline is created, a log ingestion is created together</p> <p>Q. Which deployment mode does the log agent support?</p> <p>Whether you choose the DaemonSet type or SideCar, the Log Hub supports\u3002</p> <p>Q. Is the deployment of the log agent performed automatically by the system?</p> <p>We do not support fully automated deployment yet. Currently, the Log Hub will generate a Yaml file for deployment, so that you can make changes according to your actual scenarios. In the future we will do this through the Lambda with a Kubectl environment.</p>"},{"location":"designs/eks-log/process-design/","title":"EKS Cluster Log Analytics Process","text":"<p>This document is about the Process Design for Import EKS Cluster, Create EKS Cluster Log Pipeline and Ingestion. </p>"},{"location":"designs/eks-log/process-design/#overview","title":"Overview","text":""},{"location":"designs/eks-log/process-design/#import-an-eks-cluster","title":"Import an EKS Cluster","text":""},{"location":"designs/eks-log/process-design/#collect-control-plane-logging","title":"Collect Control plane logging","text":""},{"location":"designs/eks-log/process-design/#request-to-create-pipeline-and-ingestion-for-collecting-eks-cluster-application-logs","title":"Request to create pipeline and ingestion for collecting EKS cluster application logs","text":"<p>Sequence diagram: Request to create pipeline</p> <p></p> <p>Sequence diagram: The StepFunction Process</p> <p></p> <p>Sequence diagram: Create an ingested  from an existing pipeline</p> <p></p>"},{"location":"designs/service-log/api-design/","title":"Service Log Pipeline API Design","text":""},{"location":"designs/service-log/api-design/#overview","title":"Overview","text":"<p>This document is about the API Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/api-design/#service-pipeline-apis","title":"Service Pipeline APIs","text":"<p>Service Pipeline APIs are a list of operations to manage end to end Log analytics pipelines for AWS services.</p>"},{"location":"designs/service-log/api-design/#create-service-pipeline","title":"Create Service Pipeline","text":"<p>Type: Mutation</p> <p>Description:  Create a record in DynamoDB, start an execution of Step function,  trigger CloudFormation template to run</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description type String Yes Allowed values: S3AccessLog, CloudTrail, CloudFront parameters K-V Yes Source info (such as S3 bucket, prefix) tags K-V No Tag the pipeline <p>Request:</p> <pre><code>{\n  createServicePipeline(\n    Type: S3, \n    source: \"aws-lambda-12843845950\", \n    target: \"dev\", \n    tags: [{key: \"Hello\", value: \"World\"}], \n    parameters: [\n    {parameterKey: \"engineType\", parameterValue: \"OpenSearch\"},\n    {parameterKey: \"logBucketName\", parameterValue: \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"},\n    {parameterKey: \"logBucketPrefix\", parameterValue: \"AWSLogs/347283850106/s3/aws-lambda-12843845950\"},\n    {parameterKey: \"endpoint\", parameterValue: \"vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com\"}\n    {parameterKey: \"domainName\", parameterValue: \"dev\"},\n    {parameterKey: \"indexPrefix\", parameterValue: \"aws-lambda-12843845950\"},\n    {parameterKey: \"createDashboard\", parameterValue: \"Yes\"},\n    {parameterKey: \"vpcId\", parameterValue: \"vpc-0e172e182aa53806b\"},\n    {parameterKey: \"subnetIds\", parameterValue: \"subnet-09f0654b6db09eb23,subnet-0b873d0b6e73c2f9c\"},\n    {parameterKey: \"securityGroupId\", parameterValue: \"sg-0a55e5364049a5b1d\"},\n    {parameterKey: \"backupBucketName\", parameterValue: \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"},\n    {parameterKey: \"daysToWarm\", parameterValue: \"0\"},\n    {parameterKey: \"daysToCold\", parameterValue: \"0\"},\n    {parameterKey: \"daysToRetain\", parameterValue: \"0\"}\n    ])\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"createServicePipeline\": \"24483703-41b6-43ba-aae3-19318bdb1b4e\"\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#delete-service-pipeline","title":"Delete Service Pipeline","text":"<p>Type: Mutation</p> <p>Description:  mask the record in DynamoDB as Inactive, start an execution of Step function,  trigger CloudFormation template to delete</p> <p>Resolver: Lambda</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Pipeline Unique ID in DynamodB <p>Simple Request &amp; Response:</p> <p>Request:</p> <pre><code>mutation example {\n  deleteServicePipeline(id: \"24483703-41b6-43ba-aae3-19318bdb1b4e\")\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"deleteServicePipeline\": \"OK\"\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#list-service-pipelines","title":"List Service Pipelines","text":"<p>Type: Query</p> <p>Description:  List all pipelines</p> <p>Resolver: DynamoDB</p> <p>Parameters:</p> Name Type Required Default Description page Int No 1 page number, start from 1 count String No 20 number of records per page <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  listServicePipelines(page: 1, count: 20) {\n    pipelines {\n      createdDt\n      id\n      source\n      status\n      target\n      type\n    }\n    total\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"listServicePipelines\": {\n      \"pipelines\": [\n        {\n          \"createdDt\": \"2021-09-16T04:12:06.288536\",\n          \"id\": \"f2272d96-5cb5-4eed-9d1e-bbe545cfa181\",\n          \"source\": \"abc-bucket\",\n          \"status\": \"ACTIVE\",\n          \"target\": \"dev\",\n          \"type\": \"S3\"\n        },\n        {\n          \"createdDt\": \"2021-09-16T04:12:04.216817\",\n          \"id\": \"f2b845fa-be44-4b94-9912-e692d2bc270d\",\n          \"source\": \"bcd-bucket\",\n          \"status\": \"ERROR\",\n          \"target\": \"dev\",\n          \"type\": \"S3\"\n        },\n        ...\n      ],\n      \"total\": 166\n    }\n  }\n}\n</code></pre>"},{"location":"designs/service-log/api-design/#get-service-pipeline","title":"Get Service Pipeline","text":"<p>Type: Query</p> <p>Description:  Get service pipeline detail by ID</p> <p>Resolver: DynamoDB</p> <p>Parameters:</p> Name Type Required Default Description id String Yes Unique pipeline ID <p>Simple Request &amp; Response:</p> <p>Request</p> <pre><code>query example {\n  getServicePipeline(id: \"d3b88a26-38ab-430a-9b20-1dd009586e22\") {\n    createdDt\n    id\n    parameters {\n      parameterKey\n      parameterValue\n    }\n    status\n    tags {\n      key\n      value\n    }\n    type\n    source\n    target\n    error\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"data\": {\n    \"getServicePipeline\": {\n      \"createdDt\": \"21-12-09T07:54:17Z\",\n      \"id\": \"d3b88a26-38ab-430a-9b20-1dd009586e22\",\n      \"parameters\": [\n        {\n          \"parameterKey\": \"engineType\",\n          \"parameterValue\": \"OpenSearch\"\n        },\n        {\n          \"parameterKey\": \"logBucketName\",\n          \"parameterValue\": \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"\n        },\n        {\n          \"parameterKey\": \"logBucketPrefix\",\n          \"parameterValue\": \"AWSLogs/347283850106/s3/aws-lambda-12843845950\"\n        },\n        {\n          \"parameterKey\": \"endpoint\",\n          \"parameterValue\": \"vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com\"\n        },\n        {\n          \"parameterKey\": \"domainName\",\n          \"parameterValue\": \"dev\"\n        },\n        {\n          \"parameterKey\": \"indexPrefix\",\n          \"parameterValue\": \"aws-lambda-12843845950\"\n        },\n        {\n          \"parameterKey\": \"createDashboard\",\n          \"parameterValue\": \"Yes\"\n        },\n        {\n          \"parameterKey\": \"vpcId\",\n          \"parameterValue\": \"vpc-0e172e182aa53806b\"\n        },\n        {\n          \"parameterKey\": \"subnetIds\",\n          \"parameterValue\": \"subnet-09f0654b6db09eb23,subnet-0b873d0b6e73c2f9c\"\n        },\n        {\n          \"parameterKey\": \"securityGroupId\",\n          \"parameterValue\": \"sg-0a55e5364049a5b1d\"\n        },\n        {\n          \"parameterKey\": \"backupLogBucketName\",\n          \"parameterValue\": \"loghub-loghubloggingbucket0fa53b76-1cf5iuchzpbz8\"\n        },\n        {\n          \"parameterKey\": \"daysToWarm\",\n          \"parameterValue\": \"0\"\n        },\n        {\n          \"parameterKey\": \"daysToCold\",\n          \"parameterValue\": \"0\"\n        },\n        {\n          \"parameterKey\": \"daysToRetain\",\n          \"parameterValue\": \"0\"\n        }\n      ],\n      \"status\": \"ERROR\",\n      \"tags\": [\n        {\n          \"key\": \"Hello\",\n          \"value\": \"World\"\n        }\n      ],\n      \"type\": \"S3\",\n      \"source\": \"aws-lambda-12843845950\",\n      \"target\": \"dev\",\n      \"error\": \"An error occurred (ValidationError) when calling the CreateStack operation: Parameters: [failedLogBucket] must have values\"\n    }\n  }\n}\n</code></pre>"},{"location":"designs/service-log/component-design/","title":"Service Log Pipeline Component Design","text":""},{"location":"designs/service-log/component-design/#overview","title":"Overview","text":"<p>Service Log Analytics Pipeline, as one component of Log Hub solution, is used to collect logs for AWS services, process and ingest into Amazon OpenSearch Service (AOS). This document is to describe this component is designed.</p> <p>Currently, this solution supports S3 Access Logs, CloudTrail Logs, ELB Logs, CloudFront Logs, RDS Logs, WAF Logs, Lambda Logs. To learn how to extend this solution to support more service logs, please check Tutorial: Extend Service Logs</p> <p>Info</p> <p>For more information about solution overall design, refer to High Level Design.</p>"},{"location":"designs/service-log/component-design/#component-design","title":"Component Design","text":""},{"location":"designs/service-log/component-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>Based on different log destinations, Different architectures are used. </p> <ul> <li> <p>Destination on Amazon S3</p> <p>Normally the logs on Amazon S3 are not for real-time analysis. Currently, this solution supports CloudTrail logs, CloudFront Standard logs, Amazon S3 Access logs, Elastic Load Balancing (ELB) logs, VPC Flow logs, and Amazon Config logs.</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>AWS Services store logs on Amazon S3 bucket</p> </li> <li> <p>A notification is sent to Amazon SQS when new log file is created</p> </li> <li> <p>Amazon SQS triggers the Lambda (Log processor) to run</p> </li> <li> <p>The Log processor read and processes the log file and ingest the logs into Amazon OpenSearch service.</p> </li> </ol> <p>For cross-account log ingestion, the AWS Services store logs on Amazon S3 bucket in one account, and other resources remain in Log Hub's Account:</p> <p></p> </li> <li> <p>Destination on CloudWatch Logs</p> <p>Some services can only choose Amazon CloudWatch Log Group as destination. Currently, this solution supports RDS logs and Lambda Logs</p> <p></p> <p>The process is described as below:</p> <ol> <li> <p>AWS Services store logs on Amazon CloudWatch log group</p> </li> <li> <p>The CloudWatch logs is streaming to Amazon Kinesis Data Stream (KDS) via subscription. </p> </li> <li> <p>KDS triggers the Lambda (Log processor) to run</p> </li> <li> <p>The Log processor read and processes the log records and ingest the logs into Amazon OpenSearch service.</p> </li> </ol> <p>For cross-account log ingestion, the AWS Services store logs on Amazon CloudWatch log group in one account, and other resources remain in Log Hub's Account:</p> <p></p> </li> </ul>"},{"location":"designs/service-log/component-design/#process-design","title":"Process Design","text":"<p>To learn more information about how the detail process are designed, please refer to Process Design</p>"},{"location":"designs/service-log/component-design/#api-design","title":"API Design","text":"<p>A list of GraphQL APIs are built on AWS Appsync service to support service pipeline management from Log Hub Web Console.</p> <p>To learn more information about how the backend APIs are designed, please refer to API Design</p>"},{"location":"designs/service-log/component-design/#data-model-design","title":"Data Model Design","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database to store information about the service log pipelines.</p> <p>To learn more information about how the data model is designed, please refer to Data Model Design</p>"},{"location":"designs/service-log/component-design/#cloudformation-design","title":"CloudFormation Design","text":"<p>This component can be launched independently via CloudFormation without the Solution Web Console (UI).</p> <p>The parameters in the CloudFormation template are listed as below:</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket name which stores the service logs. Log Bucket Prefix <code>&lt;Requires input&gt;</code> The S3 bucket path prefix which stores the service logs. Engine Type OpenSearch The engine type of the OpenSearch. Select OpenSearch or Elasticsearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> The domain name of the Amazon OpenSearch cluster. OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. e.g. <code>vpc-your_opensearch_domain_name-xcvgw6uu2o6zafsiefxubwuohe.us-east-1.es.amazonaws.com</code> Index Prefix <code>&lt;requires input&gt;</code> The common prefix of OpenSearch index for the log. The index name will be -elb-. Create Sample Dashboard Yes Whether to create a sample OpenSearch dashboard. VPC ID <code>&lt;requires input&gt;</code> Select a VPC which has access to the OpenSearch domain. The log processing Lambda will be resides in the selected VPC. Subnet IDs <code>&lt;requires input&gt;</code> Select at leasts two subnets which has access to the OpenSearch domain. The log processing Lambda will resides in the subnets. Please make sure the subnets has access to the Amazon S3 service. Security Group ID <code>&lt;requires input&gt;</code> Select a Security Group which will be associated to the log processing Lambda. Please make sure the Security Group has access to the OpenSearch domain. Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes, keep the size of each shard between 10-50 GiB. Number of Replicas 1 The number of days required to move the index into warm storage, this is only effecitve when the value is &gt;0 and warm storage is enabled in OpenSearch. Days to Warm Storage 0 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Cold Storage 0 The number of days required to move the index into cold storage, this is only effecitve when the value is &gt;0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index, if value is 0, the index will not be deleted."},{"location":"designs/service-log/component-design/#appendix","title":"Appendix","text":""},{"location":"designs/service-log/component-design/#service-log-output-destination","title":"Service Log Output Destination","text":"<p>Most of AWS Services output logs to Amazon CloudWatch Logs or Amazon S3, and some output to Kinesis Data Streams or Kinesis Firehose. The following table is a sample list of AWS services and their log destinations.</p> Service log destination AWS Services Amazon S3 CloudTrail, S3 Access Log, CloudFront Standard Logs, ELB Access Log, VPC Flow Logs, WAF Log, Config Log Amazon CloudWatch Logs RDS, Lambda, Lambda@Edge, VPC Flow Logs, AppSync, API Gateway, WAF Log Kinesis Firehose WAF Log Kinesis Data Streams CloudFront Real-time logs, Amazon Pinpoint events"},{"location":"designs/service-log/data-model-design/","title":"Service Log Pipeline Data Model Design","text":""},{"location":"designs/service-log/data-model-design/#overview","title":"Overview","text":"<p>This component uses Amazon DynamoDB as the backend NoSQL database. This document is about the Data Model Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/data-model-design/#service-pipeline-table","title":"Service Pipeline Table","text":"<p>Service pipeline table stores information about the service log pipelines managed by this solution.</p> <p>The data attributes are listed as below:</p> Attribute name Type Example Description Comments id String 06e3e64d-0958-43b1-b426-fe52ac55738f Unique ID of a Pipeline Partition key stackName String LogHub-Pipe-06e3e Service Pipeline CloudFormation Stack Name stackId String arn:aws:cloudformation:us-east-1:123456789012:stack/LogHub-Pipe-06e3e/a3d66790-6300-11ec-9ef5-0a829481a42d Service Pipeline CloudFormation Stack ID source String test-bucket Source of the Log target String dev OpenSearch domain status String ACTIVE Status of the pipeline error String Error message of the pipeline type String S3 AWS service type parameters List [{...}] Parameter key-value pairs to deploy the pipeline createdDt String 2021-12-22T08:24:53Z Creation date of the pipeline tags List [{...}] Custom tags"},{"location":"designs/service-log/process-design/","title":"Service Log Pipeline Process Design","text":""},{"location":"designs/service-log/process-design/#overview","title":"Overview","text":"<p>This document is about the Process Design for Service Log Pipeline component. To learn more information about the component, refer to Component Design</p>"},{"location":"designs/service-log/process-design/#service-log-pipeline-process","title":"Service Log Pipeline Process","text":"<p>At a high level, an end to end log analytics pipeline consists the 4 following stages.</p> <ol> <li>Collect </li> <li>Buffer</li> <li>Process</li> <li>Visualize</li> </ol> <p>Different services are used in different stages.</p> <p></p>"},{"location":"designs/service-log/process-design/#collect","title":"Collect","text":"<p>Customers can enable the service logs from AWS Management Console or API calls. The log is stored in different destinations.</p> <p>The main services in this stage are:</p> <ul> <li>Amazon S3</li> <li>CloudWatch Log Group</li> </ul> <p>Info</p> <p>Log Hub Solution also supports automatically enable the logs for some services, such as S3 Access Logs. Check API Design</p>"},{"location":"designs/service-log/process-design/#buffer","title":"Buffer","text":"<p>The buffering layers involves different services based on various cases.</p> <ul> <li> <p>Amazon SQS</p> <p>For service logs that is stored in Amazon S3, SQS is used as the buffering layer to receive S3 Events.</p> </li> <li> <p>Kinesis Data Stream (KDS)</p> <p>KDS can be used to subscribe the log streams from CloudWatch Logs. Also, CloudFront real-time logs can only be sent to KDS.</p> </li> <li> <p>Kinesis Data Firehose (KDF)</p> <p>KDF can be used as a bufferring layer and sink the logs to destination such as S3 buckets or directly to OpenSearch. </p> </li> </ul>"},{"location":"designs/service-log/process-design/#process","title":"Process","text":"<p>This stage uses AWS Lambda as the core service.</p> <p>The general purpose of a Log Processor is to parse the raw logs, filter and enrich the log info before ingest that to OpenSearch.</p> <p>The process includes below four steps:</p> <ol> <li>Decompress: This is only required if the source log file is compressed.</li> <li>Parse: This is to parse the raw log records such as using regex.</li> <li>Filter: This is to filter on the logs based on certain conditions.</li> <li>Enrich: This is to enrich the original log messages with extra information, such as IP to Location.</li> </ol> <p>Warning</p> <p>Currently, Filter and Enrich are not yet supported.</p> <p>The processed logs can then be ingested into AOS. There are several actions to be taken in OpenSearch for Log Ingestion.</p> <ul> <li> <p>Create Index Template</p> <p>Index templates let you initialize new indices with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of these indices have the same number of shards and replicas.</p> <p>This is very important as once the data is loaded into OpenSearch, the mapping and the number of shards can't be changed.</p> <p>Use below OpenSearch REST API to create index template <pre><code>PUT _index_template/&lt;index-template-name&gt;\n</code></pre></p> <p>Info</p> <p>This is just an one time action that is executed during the start of the CloudFormation deployment.</p> </li> <li> <p>Create Index State Management (ISM) Policy</p> <p>ISM lets you automate periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM, you can define policies that automatically handle index transition (such as hot to warm, warm to cold) or deletions to fit your use case.</p> <p>Below OpenSearch REST API is used to check if index template exists <pre><code># for OpenSearch\nPUT _opendistro/_ism/policies/{policy_id}\n\n# for Elasticsearch\nPUT _plugins/_ism/policies/{policy_id}\n</code></pre></p> <p>Info</p> <p>This is just an one time action that is executed during the start of the CloudFormation deployment.</p> </li> <li> <p>Check if index template exists</p> <p>It's a good practice to have a check whether index template already exists or not before loading data into OpenSearch.  Otherwise, the logs could be loaded as dirty data and it took more time to delete and reprocess the logs.</p> <p>Below OpenSearch REST API is used to check if index template exists <pre><code>HEAD _index_template/&lt;index-template-name&gt;\n</code></pre></p> <p>Info</p> <p>This action is executed everytime before loading data.</p> </li> <li> <p>Bulk Load</p> <p>Once the index template is created, and the data is ready to load, normally the log is loaded in batches via the Bulk load API.</p> <p>The default batch size is <code>10000</code> records. Too many records in one single batch can result in <code>513 Payload too large</code> error.</p> <p>Below OpenSearch REST API is used to load data in batches <pre><code>PUT &lt;index-name&gt;/_bulk\n</code></pre></p> </li> </ul>"},{"location":"designs/service-log/process-design/#visualize","title":"Visualize","text":"<p>Once the log data is ingested into OpenSearch, customer can then analyze and visulize the logs in OpenSearch Dashboards.</p> <p>This solution is shipped with simple dashboards for each services.</p> <ul> <li> <p>Import pre-built dashboards</p> <p>Below OpenSearch Dashboards REST API is used to import pre-built dashboards. <pre><code># for OpenSearch\nPOST _dashboards/api/saved_objects/_import?createNewCopies=true\n\n# for OpenSearch\nPOST _plugin/kibana/api/saved_objects/_import?createNewCopies=true\n</code></pre></p> </li> <li> <p>Dashboard Design</p> <p>The dashboard should contains valuable data insights. </p> <p>Take CloudFront Log as an example, the dashboards contains information such as:</p> <ul> <li>PV/UV count</li> <li>Cache Hit/Miss Rate</li> <li>Bandwidth</li> <li>Health Status (2xx, 3xx, 4xx, 5xx)</li> <li>Top Request URIs</li> <li>Top Client IPs</li> </ul> </li> </ul>"},{"location":"designs/service-log/process-design/#pipeline-orchestration-process","title":"Pipeline Orchestration Process","text":"<p>Customer can manage Service Log pipeline from Log Hub Web Console.  Below is the high-level process that is used to orchestrate service log pipeline flow.</p>"},{"location":"designs/service-log/process-design/#create-service-pipeline","title":"Create Service Pipeline","text":"<p>The process to create service log pipeline is described in below diagram:</p> <p></p> <p>The UML diagram is shown in below:</p> <p></p> <ol> <li>Create Service Pipeline API call is sent to Appsync</li> <li>Appsync invoke Lambda (pipeline handler) as resolver</li> <li>Lambda generate a UUID and create a new item in DynamoDB</li> <li>Lambda trigger Pipeline Step function to flow</li> <li>Pipeline Step Function execute CfnFlow step function as a Child Step</li> <li>CfnFlow use CloudFormation createStack api to start deployment of sub stack template</li> <li>CfnFlow use CloudFormation describe api to query the status of the sub stack</li> <li>CfnFlow check the status and repeat step 7 until the status is completed</li> <li>CfnFlow notify the result to parent Pipeline Step function flow</li> <li>Pipeline Step Function update the status to DynamoDB</li> </ol> <p>References:</p> <ul> <li>Create Service Pipeline API</li> <li>Service Pipeline Table</li> </ul>"},{"location":"designs/service-log/process-design/#delete-service-pipeline","title":"Delete Service Pipeline","text":"<p>The process to delete service log pipeline is described in below diagram:</p> <p></p> <p>The UML diagram is shown in below:</p> <p></p> <ol> <li>Delete Service Pipeline API call is sent to Appsync</li> <li>GraphQL invoke Lambda (pipeline handler) as resolver</li> <li>Lambda query item in DynamoDB to get sub stack ID</li> <li>Lambda update item in DynamoDB with status \u2018DELETING\u2019</li> <li>Lambda trigger Pipeline Step function to flow</li> <li>Pipeline Step Function execute CfnFlow step function as a Child Step</li> <li>CfnFlow use CloudFormation deleteStack api to start deletion of sub stack template</li> <li>CfnFlow use CloudFormation describe api to query the status of the sub stack</li> <li>CfnFlow check the status and repeat step 8 until the status is completed</li> <li>CfnFlow notify the result to parent Pipeline Step function flow</li> <li>Pipeline Step Function update the status to DynamoDB</li> </ol> <p>References:</p> <ul> <li>Delete Service Pipeline API</li> <li>Service Pipeline Table</li> </ul>"},{"location":"designs/service-log/tutorial-extend-service-log/","title":"Tutorial: Extend Service Logs","text":""},{"location":"designs/service-log/tutorial-extend-service-log/#overview","title":"Overview","text":"<p>The purpose of this document is to describe how to extend current Log Hub solution to support more types of AWS service logs.</p> <p>In this guide, we will work you through a tutorial of how to add support for a new type of logs using WAF Logs as an example. If you are not from CSDC solution team, we also welcome your contributions. You can jump to How to Contribute for more details.</p> <p>Notice</p> <p>So far, this guide is only for Log Destination as S3. Check more information about AWS Service Log Destination.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Clone the repo</li> <li>Provision an OpenSearch domain in VPC with private subnets for testing</li> </ul> <p>Info</p> <p>Currently, clone the code repo from code.amazon.com, all changes need be submitted to develop branch for code review.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#how-to-extend","title":"How to Extend","text":"<p>Follow below step by step guide to learn how to extend service logs analysis for WAF in Log Hub solution.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-1-update-log-processor","title":"Step 1: Update Log Processor","text":"<p>To add a new log type for WAF Logs to Log Processor Lambda,  open <code>source/constructs/lambda/pipeline/service/log-processor/util/log_parser.py</code></p> <p>Add an implemetation of LogType for WAF logs, which basically is just to implement the method <code>parse(line)</code> and return a processed record(s) in Json format.</p> <pre><code>class WAF(LogType):\n\"\"\"An implementation of LogType for WAF Logs\"\"\"\n\n    _format = \"json\"\n\n    def parse(self, line: str):\n        json_record = json.loads(line)\n\n        # Extract web acl name, host and user agent\n        json_record[\"webaclName\"] = re.search(\n            \"[^/]/webacl/([^/]*)\", json_record[\"webaclId\"]\n        ).group(1)\n        headers = json_record[\"httpRequest\"][\"headers\"]\n        for header in headers:\n            if header[\"name\"].lower() == \"host\":\n                json_record[\"host\"] = header[\"value\"]\n            elif header[\"name\"].lower() == \"user-agent\":\n                json_record[\"userAgent\"] = header[\"value\"]\n            else:\n                continue\n        return json_record\n</code></pre> <p>Info</p> <p>WAF Logs are in Json format, check Appendix 1: Log Type Implementation for another example for flat log files.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-2-add-cdk-stack","title":"Step 2: Add CDK Stack","text":"<p>To generate a new CloudFormation Template for Waf Logs,  Open <code>source/constructs/bin/main.ts</code> , add a new line as below:</p> <pre><code>new ServiceLogPipelineStack(app, 'WAFLog', { logType: 'WAF' });\n</code></pre> <p>For WAF, simply reuse the common CloudFormation parameters. Open <code>Log-hub/source/constructs/lib/pipeline/service/service-log-pipeline-stack.ts</code>, and add \u2018WAF\u2019 to below line (This is required for all logs that is with log destination as S3):</p> <pre><code>if (['S3', 'CloudTrail', 'ELB', 'CloudFront', 'WAF'].includes(props.logType)) {\nconst logBucketName = new CfnParameter(this, 'logBucketName', {\n...\n</code></pre> <p>For some other log types, you can add more CloudFormation parameters if needed. For example, add an extra parameter of <code>Log Format</code> for VPC Flow Logs since the log format can be customized.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-3-add-index-template","title":"Step 3: Add Index Template","text":"<p>Index templates let you initialize new indices with predefined mappings and settings. Before logs are ingested to OpenSearch, the index template/mapping must exist, otherwise, once data is loaded, the index mapping can't be changed.</p> <p>Log Type is used to identify which template json file to use, Make sure the file name is same as the log type in lower case. </p> <p>For example, if the log Type is \u2018WAF\u2019,  create a waf.json in folder <code>source/constructs/lambda/pipeline/common/opensearch-helper/assets/index_template</code></p> <p>The template must be in a format of : <pre><code>{\n\"aliases\": {},\n\"mappings\": {\n\"properties\": {\n\"...\": {\n\"type\": \"...\",\n...\n},\n...\n}\n},\n\"settings\": {\n\"index\": {\n\"number_of_shards\": \"5\",\n\"number_of_replicas\": \"1\"\n}\n}\n}\n</code></pre></p> <p>Keep aliases as blank. Make sure at least number_of_shards and number_of_replicas are in the setting section (keep 5 and 1 as the default value). The alias and setting will be overrided during deployment.</p> <p>Info</p> <p>To learn more about index template, check Official OpenSearch Document about Index Template</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-4-cdk-deploy","title":"Step 4: CDK Deploy","text":"<p>Run cdk deploy, for example:</p> <pre><code>cdk deploy WAFLog \\                                       \n--parameters vpcId=vpc-0e172e182aa53806b \\\n--parameters subnetIds=subnet-06548d0c4ee34da59,subnet-0b873d0b6e73c2f9c \\\n--parameters securityGroupId=sg-04b03782612cb485f \\\n--parameters endpoint=vpc-dev-ardonphnbg327lwqncuj2vps3q.eu-west-1.es.amazonaws.com \\\n--parameters domainName=dev \\\n--parameters logBucketName=loghub-alb-loggingbucket0fa53b76 \\\n--parameters logBucketPrefix=waf \\\n--parameters backupBucketName=loghub-alb-loggingbucket0fa53b76 \\\n--parameters createDashboard=No \\\n--parameters indexPrefix=xxxxxx \\\n--parameters engineType=OpenSearch\n</code></pre> <p>To learn more about what the parameters are, check CloudFormation Design</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-5-test-and-verify","title":"Step 5:  Test and Verify","text":"<p>Generate some log data by making some requests to WAF (For example, if WAF is for cloudfront, simply access the cloudfront link). Check S3 bucket to see if the log files are generated.</p> <p>Then open Lambda in AWS management console, find and open the function <code>WAFLog-WAFLogPipelineLogProcessorFn</code> and check the cloudwatch logs. If the ingestion is successful, you should see logs as the following:</p> <pre><code>...\n[INFO]  2022-01-14T05:45:14.397Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; bulk_load response code 200\n[INFO]  2022-01-14T05:45:15.165Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    PUT xxxxxx-waf-2022-01-14/_bulk\n[INFO]  2022-01-14T05:45:16.163Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; bulk_load response code 200\n[INFO]  2022-01-14T05:45:16.229Z    947eb9f3-a2c6-4d59-bf63-870be02dd20a    --&gt; Total: 278126 Success: 278126 Fail: 0\nEND RequestId: 947eb9f3-a2c6-4d59-bf63-870be02dd20a\n</code></pre> <p>You can also see error messages if the ingestion failed, check and then fix the error accordingly.</p>"},{"location":"designs/service-log/tutorial-extend-service-log/#step-6-export-sample-dashboards","title":"Step 6: Export Sample Dashboards","text":"<p>Once data is ingested into OpenSearch, you can then create dashboard and visualize for the logs. Follow the same naming standard.</p> <p>For example,  if the index prefix is <code>xxxxxx-waf-*</code>,  create everything with same prefix <code>xxxxxx-waf-</code></p> <p></p> <p>Once dashboard is completed,  export the dashboard with related objects, the exported file is with file extention <code>.ndjson</code>. Open the export ndjson, and replace all <code>xxxxxx-waf</code> to <code>%%INDEX%%</code>, then save the file to project folder <code>source/constructs/lambda/pipeline/common/opensearch-helper/assets/saved_objects</code></p>"},{"location":"designs/service-log/tutorial-extend-service-log/#how-to-contribute","title":"How to Contribute","text":"<p>If you are not one of the solution team members, and you want to contribute for new service logs, please follow the section How to Extend and get it built and tested in your account.</p> <p>After that, you can contact one of our team members, and submit us with below artifacts:</p> <ul> <li>Log Type implementation in Python</li> <li>Index Template file (.json)</li> <li>Sample Dashboard file (.ndjson)</li> <li>Some sample log file that we can test (Nice to have)</li> </ul>"},{"location":"designs/service-log/tutorial-extend-service-log/#appendix","title":"Appendix","text":""},{"location":"designs/service-log/tutorial-extend-service-log/#log-type-implementation","title":"Log Type Implementation","text":"<p>The Log Type definition</p> <pre><code>class LogType(ABC):\n\"\"\"An abstract class represents one type of Logs.\n\n    Each AWS service has its own log format.\n    Create a class for each service with an implementation of `parse(line)` to parse its service logs\n    \"\"\"\n\n    _fields = []  # list of fields\n    _format = \"text\"  # log file format, such as json, text, etc.\n\n    @abstractmethod\n    def parse(self, line: str):\n\"\"\"Parse the original raw log record, and return processed json record(s).\n\n        This should be implemented in each service class.\n        \"\"\"\n        pass\n\n    ...\n</code></pre> <p>Below is an example of implementation for ELB (ALB) Logs</p> <pre><code>class ELB(LogType):\n\"\"\"An implementation of LogType for ELB Logs\"\"\"\n\n    _fields = [\n        \"type\",\n        \"timestamp\",\n        \"elb\",\n        \"client_ip\",\n        \"client_port\",\n        \"target_ip\",\n        ...\n    ]\n\n    def parse(self, line: str) -&gt; dict:\n        json_record = {}\n        pattern = (\n            \"([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) \"\n            '([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \"([^ ]*) ([^ ]*) '\n            '(- |[^ ]*)\" \"([^\"]*)\" ([A-Z0-9-]+) ([A-Za-z0-9.-]*) ([^ ]*) \"([^\"]*)\" \"([^\"]*)\" '\n            '\"([^\"]*)\" ([-.0-9]*) ([^ ]*) \"([^\"]*)\" \"([^\"]*)\" \"([^ ]*)\" \"([^ ]+?)\" '\n            '\"([^ ]+)\" \"([^ ]*)\" \"([^ ]*)\"'\n        )\n        result = re.match(pattern, line)\n        if result:\n            for i, attr in enumerate(self._fields):\n                # print(f'{attr} = {result.group(i+1)}')\n                json_record[attr] = result.group(i + 1)\n        else:\n            logger.error(\"Unable to parse line: %s\", line)\n        return json_record\n</code></pre>"},{"location":"designs/solution/high-level-design/","title":"Log Hub Solution High Level Design","text":""},{"location":"designs/solution/high-level-design/#overview","title":"Overview","text":"<p>Log Hub is a solution that enables customer to easliy and quickly build end-to-end log analytics pipelines on top of Amazon OpenSearch Service (AOS). A log pipeline includes a series of log processing steps, including collecting logs from source, processing and sending them to OpenSearch as destination for further analysis.</p> <p>This solution provides a web management console from which customer can easliy manage log analytics pipelines and gain valuable data insights for both AWS service logs and application logs with out of the box visualization dashboards without worring about the underling technical complexity. </p> <p>The purpose of this document is to descibe how Log Hub solution is technical designed from high level perspective.</p>"},{"location":"designs/solution/high-level-design/#requirements","title":"Requirements","text":""},{"location":"designs/solution/high-level-design/#functional-requirements","title":"Functional Requirements","text":"<p>This solution is designed with below functional requirements:</p> <ul> <li>A centralized web console for customer to manage all the tasks to reduce operational complexities.</li> <li>Supports both AWS service logs and application logs</li> <li>Supports automatically creating visualization dashboards</li> <li>Supports log lifecycle management</li> <li>Supports cross accounts/cross regions log management</li> <li>Supports both China and Global regions</li> </ul>"},{"location":"designs/solution/high-level-design/#non-functional-requirements","title":"Non-Functional Requirements","text":"<p>The design must meet below non-functional requirements:</p> <ul> <li> <p>Security: Authentication and Authorization is required to protect unexpected access.</p> </li> <li> <p>Scalability: The design must be able to support different scales of logs.</p> </li> <li> <p>Stability: The design must support auto-retries for recoverable errors.</p> </li> <li> <p>Cost Effective: Use serverless architecture design whenever possible, and support log life-cycle management to reduce the overall costs.</p> </li> </ul>"},{"location":"designs/solution/high-level-design/#assumptions","title":"Assumptions","text":"<p>The design is based on below assumptions:</p> <ul> <li> <p>Customers who use this solution must be with administrator access on AWS to be able to perform related functions, as this solution will provision different resources such as Lambda, DynamoDB, etc in the AWS account.</p> </li> <li> <p>Customers must understand their business requirments for log analysis, such as the volume of the logs, the days to retain logs etc.</p> </li> </ul>"},{"location":"designs/solution/high-level-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>Below is the high level architecture diagram:</p> <p></p> <p>This solution deploys the following infrastructure in your AWS Cloud account:</p> <ol> <li> <p>Amazon CloudFront to distribute the frontend web UI assets hosted in Amazon S3 bucket.</p> </li> <li> <p>AWS AppSync to provide the backend GraphQL APIs.</p> </li> <li> <p>Amazon Cognito user pool to provide authentication and authorization for frontend and backend.</p> </li> <li> <p>Amazon DynamoDB as backend database to store the solution related information.</p> </li> <li> <p>AWS Lambda to interact with other AWS Services to execute core logic including managing log pipelines or managing log agents and get the information updated in DynamoDB tables.</p> </li> <li> <p>AWS Step Functions to orchestrate on-demand AWS CloudFormation deployment of a set of predefined stacks for log pipeline management. The log pipeline stacks deploys separate AWS resources and are used to collect and process logs and ingest them into Amazon OpenSearch Service for further analysis and visualiztion.</p> </li> <li> <p>AWS Systems Manager and Amazon EventBridge to manage log agent for collecting logs from Application Servers, such as installing log agents (fluentbit) to Application servers and monitoring the health status of the agents.</p> </li> </ol>"},{"location":"designs/solution/high-level-design/#component-definition","title":"Component Definition","text":"<p>This solution consists of below main components:</p> <ul> <li> <p>Domain Management</p> <p>This solution uses Amazon OpenSearch Service as the underlying engine to store and analyze logs. This Domain Management component consists a list of operations on top of existing AOS domains, such as importing an existing AOS domain for log ingestion, providing a proxy for access the AOS dashboards which is within VPC, etc.</p> <p>Info</p> <p>To learn more about how this component is designed, please refer to Domain Management Component Design</p> <p>Warning</p> <p>Provision of AOS domain is not in scope of this component. Customer needs to create AOS domain before using this component.</p> </li> <li> <p>Service Log Pipeline</p> <p>This solution supports out of the box log analysis for many AWS service logs, such as Amazon S3 access logs, ELB access logs, etc.  This Service Log Pipeline component is designed to reduce the complexisities of building log analytics pipelines for different AWS services with different formats. Customer can collect and process AWS service logs without writing any codes, as well as gain data insights using out of the box visualization dashboards.</p> <p>Info</p> <p>To learn more about how this component is designed, please refer to Service Log Pipeline Component Design</p> </li> <li> <p>Application Log Pipeline</p> <p>This solution supports out of the box log analysis for application logs, such as Nginx/Apache logs or general application logs via regex parser. This Application Log Pipeline component uses Fluent Bit as the underlying log agent to collect logs from the application servers, and allow customers to easily install log agent and monitor the healthy of the agent via System Manager.</p> </li> </ul>"},{"location":"workshop/clean-up/","title":"Clean Up","text":""},{"location":"workshop/clean-up/#optional-delete-eks-cluster","title":"(Optional) Delete EKS Cluster","text":"<p>Info</p> <p>You need to clean up EKS only if you have setup EKS Cluster during this workshop</p> <ol> <li>Undeploy the applications. Go to the Cloud9 workspace created in Pre-request <pre><code>kubectl delete -f fluent-bit-logging.yaml\nkubectl delete -f nginx.yaml\n</code></pre></li> <li>Delete the EKS Cluster <pre><code>eksctl delete cluster --name=loghub-workshop-eks\n</code></pre></li> <li>Delete the workspace<ul> <li>Go to your Cloud9 Environment through AWS Management Console</li> <li>Select the environment named eksworkspace and delete</li> </ul> </li> </ol>"},{"location":"workshop/clean-up/#delete-stacks","title":"Delete Stacks","text":"<p>Please follow the steps to clean up all the stacks:</p> <ol> <li> <p>Go to AWS Management Console &gt; VPC. Delete the VPC peering.</p> </li> <li> <p>Go to AWS Management Console &gt; EC2. Detach created policy from Instance named <code>LoghubWorkshop/workshopASG</code></p> </li> <li> <p>Go to AWS Management Console &gt; CloudFormation. Detete all the log pipelines</p> </li> <li> <p>Delete proxy stack</p> </li> <li> <p>Delete WorkshopDemo stack</p> </li> <li> <p>Delete LogHub stack</p> </li> </ol>"},{"location":"workshop/introduction/","title":"Introduction","text":"<p>Pretend you are working as an operation engineer in a e-commercial company X, your manager asks you to build a centralized logging system, and at the same time, enable the Business Intelligence team to perform some basic data analyze functionalities through this system, like showing the top 10 popular products, etc.</p> <p>It sounds annoying, right? Because building a centralized logging system is always a time-consuming and complicated job for operation teams. Not to mention the effort to maintain high availability and low operational cost.</p> <p>Now, Log Hub can help!</p> <p>It's an AWS Solution that makes log analytics easy on AWS.</p> <p>This workshop will help you quickly understand and get hands on Log Hub solution. It will help you go through the whole process and take a glance at how much the dev-ops effort can be saved by using this solution.</p> <p>What you need to do during this workshop:</p> <ul> <li> <p>Deploy the Log Hub solution and a dummy website in your AWS account (using CloudFormation) to simulate the environment of an e-commercial website.</p> </li> <li> <p>Ingest both AWS Service logs and application logs (EKS pod logs as optional) to take a peek at possible ways of utilization.</p> </li> <li> <p>Use OpenSearch Dashboards to monitor logs and extract business values from those out-of-box dashboards.</p> </li> </ul> <p>For detailed architecture design of Log Hub, please refer to this diagram:</p> <p></p>"},{"location":"workshop/dashboard-data/generate-logs/","title":"Generate Logs","text":"<p>Warning</p> <p>Before proceeding, please make sure you have finished all the previous sections!</p> <p>You have set up 4 types of log ingestion in a web hosting user scenarios. Now, it is time to generate some logs.  </p>"},{"location":"workshop/dashboard-data/generate-logs/#generate-access-logs","title":"Generate access logs","text":"<p>Please tick the All Log-Hub Pipeline Setup Completed column in this Quip to let the support team know that you have finished creating all the log pipelines in the previous section. </p> <p>Once the support team noticed your tick, we will start to create access event from our side for you. And you will be able to see a tick on the Log Generation Started column.</p> <p>Now we can proceed to view dashboard section!</p>"},{"location":"workshop/dashboard-data/view-dashboard/","title":"View dashboard","text":"<p>We have successfully finished all the steps before viewing the dashboards.</p> <p>Now, let's explore the OpenSearch dashboard. Please open up the OpenSearch Dashboard we previously logged in.</p> <ol> <li>On the Log Hub console, select the OpenSearch Domains on the left navigation bar.</li> <li>Select the domain you have imported.</li> <li>Click the Link in General configuration &gt; Access Proxy.</li> <li> <p>Double-check your tenant by click the little circle on the right upper corner, select Switch tenants.  Check if Global is selected, then click Confirm.</p> </li> <li> <p>Now you can go and play with your dashboard, go to the location shown in the graph below, you can find several dashboards have already been imported for you. Click each one of them, and you can view all the details by yourself: </p> </li> </ol> <p>Let's see one example dashboard, the elb sample dashboard. Select workshop-elb-dashboard and change the time range, we can see elb logs has been streamed into OpenSearch already:  There are several metrics we can see, for example: we can see detailed number of total sent bytes and received bytes. </p> <p>Operation engineers can extract useful information out of it and adjust their business architecture.</p> <p>For BI team, let's see the Top Request URLs block, we can easily find out which product was the most viewed product in their website.</p> <p>The above only gives you a possible way of using the sample OpenSearch Dashboard. Customers with specific demand can even customize their own dashboard to get business insights.</p> <p>Now, you can continue play around inside the dashboard and our workshop is reaching the end.</p>"},{"location":"workshop/deployment/acm-certification/","title":"Import SSL certificate into ACM","text":"<p>Estimated time: 2 minutes</p> <p>In this section, you will import an SSL certificate into ACM. An ACM certificate is needed to create a proxy to access the OpenSearch Dashboards.  We recommended you to use your own domain and certificate. In this workshop, for your convenience, we have generated the certificate for you using Let's encrypt. Follow the instruction below to import it into ACM.  </p> <ol> <li>Go to AWS Certificate Manager Console.</li> <li>Select Import on the right-upper corner.</li> <li>Download the cert.pem, open with text editor and copy to fill in Certificate body.</li> <li>Download the privkey.pem, open with text editor and copy to fill in Certificate private key.</li> <li>Download the chain.pem, open with text editor and copy to fill in Certificate chain.</li> <li>Click Next, Next and Import. </li> </ol> <p>If you see the certification shows Issued, it means certification successfully imported. </p>"},{"location":"workshop/deployment/create-eks/","title":"Create EKS Cluster","text":"<p>Estimated time: 20 minutes</p>"},{"location":"workshop/deployment/create-eks/#must-read","title":"Must Read","text":"<ol> <li>Make sure you have one extra vacancy VPC which will be used for EKS Cluster.</li> <li>Make sure you have one extra EIP (Elastic IP address) vacancy for NAT used by EKS Cluster.</li> </ol>"},{"location":"workshop/deployment/create-eks/#create-a-workspace","title":"Create a Workspace","text":""},{"location":"workshop/deployment/create-eks/#launch-cloud9","title":"Launch Cloud9","text":"<p>Warning</p> <p>If you already have a Cloud 9 Environment. Just open the existing IDE in the Cloud9 console.</p> <ol> <li>Create a Cloud9 Environment through AWS Management Console</li> <li>Select Create environment</li> <li>Name it eksworkspace, click Next.</li> <li>Choose t3.small for instance type, take all default values and click Create environment</li> </ol> <p>When it comes up, close the welcome tab, and Open a new terminal tab.</p> <p></p> <p></p> <p>Your workspace should now look like this:</p> <p></p> <p>Upgrade AWS Cli version by running the command below in the terminal <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre></p>"},{"location":"workshop/deployment/create-eks/#grand-administratoraccess-to-the-workspace","title":"Grand AdministratorAccess to the workspace","text":"<ol> <li>Click the top-right grey circle button and select Manage EC2 Instance</li> </ol> <ol> <li>Select the instance, then choose Security Tab.</li> <li>If your EC2 have an IAM Role already, Click the IAM Role and add AdministratorAccess to the permission.</li> <li>If your EC2 does not have an IAM Role created. Create an IAM Role and attach to it.<ul> <li>Follow this link to create an IAM role with Administrator access</li> <li>Confirm that AWS service and EC2 are selected, then click Next: Permission to view permissions.</li> <li>Confirm that AdministratorAccess is checked, then click Next: Tags to assign tags.</li> <li>Take the defaults, and click Next: Review to review.</li> <li>Confirm that loghubworkshop-admin is filled in as Name, and click Create role.</li> <li>Go back to your EC2, choose Actions / Security / Modify IAM Role, and add loghubworkshop-admin</li> </ul> </li> </ol>"},{"location":"workshop/deployment/create-eks/#update-iam-settings-for-your-workspace","title":"Update IAM Settings for your workspace","text":"<p>Info</p> <p>Cloud9 normally manages IAM credentials dynamically. This isn\u2019t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.</p> <ol> <li>To ensure temporary credentials aren\u2019t already in place we will remove any existing credentials file as well as disabling AWS managed temporary credentials: <pre><code>aws cloud9 update-environment  --environment-id $C9_PID --managed-credentials-action DISABLE\nrm -vf ${HOME}/.aws/credentials\n</code></pre></li> <li>Configure our aws cli with us-east-1 as default <pre><code>aws configure set default.region us-east-1\n</code></pre></li> </ol>"},{"location":"workshop/deployment/create-eks/#install-kubernetes-tools","title":"Install Kubernetes Tools","text":"<p>Warning</p> <p>In this workshop, we will use kubectl v1.22.</p> <p>In your workspace (Cloud9 IDE), run the command below: <pre><code>curl -LO https://dl.k8s.io/release/v1.22.0/bin/linux/amd64/kubectl\nsudo chmod 755 ./kubectl\nsudo mv ./kubectl /usr/local/bin\n</code></pre></p>"},{"location":"workshop/deployment/create-eks/#install-eksctl","title":"Install eksctl","text":"<p>In your workspace (Cloud9 IDE), run the command below:</p> <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv -v /tmp/eksctl /usr/local/bin\n</code></pre>"},{"location":"workshop/deployment/create-eks/#create-an-eks-cluster","title":"Create an EKS Cluster","text":"<ol> <li> <p>In your workspace, create a new file eks.yaml.</p> <p></p> </li> <li> <p>Copy and paste the content below in eks.yaml file created above and save: <pre><code>---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\nname: loghub-workshop-eks\nregion: us-east-1\nversion: \"1.22\"\niam:\nwithOIDC: true\nmanagedNodeGroups:\n- name: workshop-nodes\ndesiredCapacity: 1\ninstanceType: m6g.large\nprivateNetworking: true\nsecurityGroups:\nattachIDs: [\"{SecurityGroup_ID}\"]\nssh:\nenableSsm: true\nvpc:\nid: \"{VPC_ID}\"\nsubnets:\nprivate:\nus-east-1a:\nid: \"{SUBNET_us_east_1a}\"\nus-east-1b:\nid: \"{SUBNET_us_east_1b}\"\n</code></pre></p> </li> <li> <p>Replace the <code>{VPC_ID}</code>, <code>{SUBNET_us_east_1a}</code>, <code>{SUBNET_us_east_1b}</code> with the value which can be found below </p> <ul> <li>Go to AWS Console &gt; VPC &gt; Subnets</li> <li> <p>Use the value of <code>VPC</code> and <code>Subnet</code>. Please be careful that the subnets are different in each availability zones.</p> <p></p> </li> </ul> </li> <li> <p>Find and replace the <code>{SecurityGroup_ID}</code>. </p> <ul> <li>Go to AWS Console &gt; OpenSearch. Which is created during Create Demo Website</li> <li> <p>Use the value of <code>Security group</code>.</p> <p></p> </li> </ul> </li> <li> <p>run the command to create EKS <pre><code>eksctl create cluster -f eks.yaml\n</code></pre></p> </li> <li>Wait till success</li> </ol>"},{"location":"workshop/deployment/create-eks/#deploy-nginx-in-eks","title":"Deploy Nginx in EKS","text":"<ol> <li>create a new file nginx.yaml with the content below and save: <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: nginx-ns\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: nginx-user\nnamespace: nginx-ns\n---        apiVersion: apps/v1\nkind: Deployment\nmetadata:\nnamespace: nginx-ns\nname: app-nginx-demo\nlabels:\napp.kubernetes.io/name: app-nginx-demo\nversion: v1\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: app-nginx-demo\nstrategy:\nrollingUpdate:\nmaxSurge: 25%\nmaxUnavailable: 25%\ntype: RollingUpdate\ntemplate:\nmetadata:\nlabels:\napp: app-nginx-demo\nspec:\nserviceAccountName: nginx-user\ncontainers:\n- image: nginx:1.20\nimagePullPolicy: Always\nname: nginx\nports:\n- containerPort: 80\nprotocol: TCP\n\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: nginx-service\nnamespace: nginx-ns\nspec:\ntype: LoadBalancer\nselector:\napp: app-nginx-demo\nports:\n- protocol: TCP\nport: 80\ntargetPort: 80\n</code></pre></li> <li>Deploy nginx <pre><code>kubectl apply -f nginx.yaml\n</code></pre></li> <li>make sure that nginx pod is running <pre><code>kubectl get pods -n nginx-ns\n</code></pre></li> </ol>"},{"location":"workshop/deployment/deploy-demo-web-site/","title":"Deploy E-Commerce Demo Site &amp; OpenSearch Domain","text":"<p>Estimated time: 20 minutes</p> <p>Warning</p> <p>Please make sure we have at least two vacancies to create new VPCs in your us-east-1 region. This workshop will automatically create two VPCs in your us-east-1 region in total, so lack of VPC limit would cause deployment failure.</p>"},{"location":"workshop/deployment/deploy-demo-web-site/#launch-stack","title":"Launch Stack","text":"<ol> <li> <p>Go to the AWS Management Console and select the button below to launch the <code>WorkshopDemo</code> AWS CloudFormation template.</p> <p></p> </li> <li> <p>We launch this template in US East (N. Virginia) Region, please check the region on the right-upper corner and make sure it's correct.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, keep the name unchanged.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box I acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create Stack to deploy the stack. The deployment process will take about 15 mins.</p> </li> </ol> <p>This Cloudformation Stack will help you automatically deploy a complete three-tier web site architecture, which consists of ALB, EC2, S3, Cloudfront, DDB and an OpenSearch inside.</p> <p>The architecture diagram is shown as follows: </p> <p>You can now view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 20 minutes. </p>"},{"location":"workshop/deployment/deploy-demo-web-site/#verify-the-demo-site-import-sample-data","title":"Verify the Demo Site &amp; Import Sample Data","text":"<p>You can now acess the front-end web page through the output of the CloudFormation and import some sample data.</p> <ol> <li>Select the CloudFormation Stack, and choose Outputs.</li> <li>Find the value of ALBCNAME and open the URL in browser.</li> <li>Click the Import Demo Data button</li> </ol> <p>The web should look like this: </p> <p>Note</p> <p>If it shows code 502, please wait for 3 more minutes to let EC2 fully booted. Then refresh the web site again. If it doesn\u2019t work, please reboot two EC2, which names are LoghubWorkshop/workshopASG, the demo site will be restarted automatically within 2 minutes. </p> <p></p>"},{"location":"workshop/deployment/deploy-log-hub/","title":"Deploy Log Hub","text":"<p>Estimated time: 15 minutes</p> <p>Warning</p> <p>Before following this section, please make sure we have one more vacancy to create new VPC, and two more vacancies to create EIP in your us-east-1 region. This cloudformation deployment will automatically create one VPC in your us-east-1 region and occupy two more EIP in total, so lack of VPC and EIP would cause deployment failure.  Further more, five new S3 buckets will be created in total. So please also make sure your S3 bucket limit has not been reached.</p>"},{"location":"workshop/deployment/deploy-log-hub/#launch-stack","title":"Launch Stack","text":"<ol> <li> <p>Log in the AWS Management Console and select the button below to launch the <code>LogHub</code> AWS CloudFormation template.</p> <p></p> </li> <li> <p>We launch this template in US East (N. Virginia) Region, please check the region on the right-upper corner and make sure it's correct.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, leave the stack name as LogHub.</p> </li> <li> <p>Under Parameters, enter the email, this email will be used as your username to login the dashboard.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes. The successful deployment should look like this: </p> <p></p>"},{"location":"workshop/deployment/deploy-log-hub/#access-the-log-hub-web-console","title":"Access the Log Hub Web Console","text":"<p>This solution will generate a CloudFront endpoint that gives you access to the Log Hub console. The endpoint can be found in Outputs section of the CloudFormation template as WebConsoleUrl. An auto-generated password will be sent to your email address, you will need it to log in to the console. Please remember to omit the last digit <code>.</code> in your email.</p> <ol> <li> <p>Open the WebConsoleUrl in the browser. You will be navigated to a sign-in page.     </p> </li> <li> <p>Input the email as the Username, and fill with the auto-generated password in the Password field.</p> <p></p> </li> <li> <p>Choose Sign in.</p> </li> <li> <p>You will be asked to change your password for the first-time login. Follow the guide to change your password.</p> </li> <li> <p>You will be asked to confirm your email address for password recovery. Skip it this time.</p> </li> </ol> <p>So far, we have successfully deployed the main stack of LogHub.</p> <p>Now you can see the LogHub Web Console, please do not close it, we will do further steps on it later. </p>"},{"location":"workshop/deployment/key-pair/","title":"EC2 Key Pair","text":"<p>Estimated time: 2 minutes</p> <p>Note</p> <p>We deploy all the things in US East (N. Virginia) Region, so please make sure you are in the correct region!</p> <p>During the deployment of Log Hub, we need a key pair to initiate several EC2 instances to host Nginx server acting as proxy for OpenSearch Dashboards. So if you are using a new AWS account, please follow the steps below to create a new key pair:</p> <ol> <li> <p>Go to AWS EC2 console &gt; Key pairs</p> </li> <li> <p>Click Create key pair on right-upper corner of the page</p> </li> <li> <p>Type your own key-pair name and create, then automated download will initiate</p> </li> </ol>"},{"location":"workshop/deployment/must-read/","title":"Must Read","text":"<p>Before you proceed to the next section, please read the following basic pre-requisites. Failure of meeting these requirements will cause the failure of deployment for sure. Make sure you do not skip or ignore any one of them:</p> <ul> <li>We deploy all the things in N. Virginia (us-east-1) Region. The workshop is intended to be work in other regions as well. But we have not tested in other regions.</li> <li>Make sure you have at least TWO vacancies to create new VPCs in your us-east-1 region. This workshop will automatically create two VPCs in the us-east-1 region.</li> <li>Make sure you have at least THREE EIP (Elastic IP addresses) vacancies. This workshop will occupy three EIP, one for E-Commerce Demo Site and the other two for OpenSearch Dashboard Proxy.</li> <li>Make sure you have at least EIGHT S3 bucket vacancies. This workshop will create eight new s3 buckets in your account.</li> </ul>"},{"location":"workshop/deployment/service-linked-role/","title":"Create service linked role for your AWS account","text":"<p>Estimated time: 2 minutes</p> <p>Note</p> <p>It's OK to skip this section if your account has already deployed OpenSearch once before. Please jump to 2.3.</p> <ol> <li> <p>Make sure you have already setup AWS credentials on your local machine.</p> </li> <li> <p>Open up a new terminal.</p> </li> <li> <p>Type in: <pre><code>aws configure\n</code></pre></p> </li> <li> <p>Please double-check the access key, secret key and region are correct for your deployment account </p> </li> <li> <p>Type in: <pre><code>aws iam create-service-linked-role --aws-service-name es.amazonaws.com\n</code></pre></p> <p>This command will add a OpenSearch service linked role for you. It gives permission to OpenSearch to launch in your new created VPC.</p> </li> <li> <p>If the output of that command shows a JSON format policy: <pre><code>{\n    \"Role\": {\n        \"Path\": \"/aws-service-role/es.amazonaws.com/\",\n        \"RoleName\": \"AWSServiceRoleForAmazonElasticsearchService\",\n        \"RoleId\": \"XXXXXXXXXXXXXXXXXX\",\n        \"Arn\": \"arn:aws:iam::XXXXXXXXXXXXX:role/aws-service-role/es.amazonaws.com/AWSServiceRoleForAmazonElasticsearchService\",\n        \"CreateDate\": \"2021-12-23T05:29:55+00:00\",\n        \"AssumeRolePolicyDocument\": {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Action\": [\n                        \"sts:AssumeRole\"\n                    ],\n                    \"Effect\": \"Allow\",\n                    \"Principal\": {\n                        \"Service\": [\n                            \"es.amazonaws.com\"\n                        ]\n                    }\n                }\n            ]\n        }\n    }\n}\n</code></pre></p> </li> </ol> <p>That means the role has been successfully created. You can proceed to deploy demo web site.</p> <p>Note</p> <p>It's OK if it shows:</p> <p><code>An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForAmazonElasticsearchService has been taken in this account, please try a different suffix.</code></p> <p>This is means you have already created the ES service linked role before.</p>"},{"location":"workshop/domain-management/access-proxy/","title":"Access AOS via Proxy","text":"<p>Estimated time: 3 minutes</p> <p>We have deployed proxys through UI in the previous section. Now, let's try to access the OpenSearch Dashboard through ALB DNS address.</p> <ol> <li>Go to Log Hub Console &gt; OpenSearch Domains &gt; workshop-os &gt; Access Proxy</li> <li> <p>Copy the Load Balancer Domain name: <code>LogHu-LoadB-XXXXXXXXXX</code> </p> <p>Note</p> <p>Please do not use the Domain name to access LogHub. In this workshop it will not work because we are entering a fake one. But for real customer cases, the domain name should be a real one.</p> </li> <li> <p>Open a new tab in your browser, type in: <code>https://&lt;Your_Copied_Load_Balancer_Domain_Name&gt;/_dashboards/</code>. </p> <p>The browser may warn you that the link you are going to is not secure.  Please just ignore the warning and choose the Advanced button.</p> <p>The following graph is an example of Chrome:  Click the revealed URL. </p> <p>The following graph is an example of FireFox:  Click Accept the Risk and Continue.</p> <p>Note</p> <p>If you still can not access, please double check if you have disabled Enhanced Protection function in your browser.</p> <p>We have this warning issue because the url we are using is not resolved in Route53. This is a expected issue and will only occur in this workshop. For real use case, customers need to use a resolved domain name and valid certification to access the dashboard.</p> <p>Now we can start login the OpenSearch Dashboard!</p> </li> <li> <p>The username is <code>admin</code> and password is <code>Loghub@@123</code>, which are both fixed, so just copy and paste.</p> </li> <li>You can now access the OpenSearch portal! Select Global on the popup window:  Then select Explore on my own\uff0cthen you can see a blank dashboard since we have not generated access log yet: </li> </ol> <p>You have completed this part and successfully access the proxy server to view the OpenSearch Dashboard! </p> <p>Just leave the dashboard open and we will go back later.</p>"},{"location":"workshop/domain-management/create-proxy/","title":"Create proxy","text":"<p>Estimated time: 15 minutes</p> <p>Note</p> <p>Please make sure you have finished the section Deployment &gt; Pre-requisites of this workshop guide before proceeding.</p> <p>By default, Amazon OpenSearch domain within VPC cannot be access from the Internet. There are a couple of ways to  access the built-in dashboard of OpenSearch using VPC. In Log Hub solution, you can deploy a Nginx based proxy to allows public access to the OpenSearch domain. The following is the architecture diagram:</p> <p></p>"},{"location":"workshop/domain-management/create-proxy/#create-a-proxy","title":"Create a proxy","text":"<ol> <li>In the navigation pane, under Clusters, choose OpenSearch domains.</li> <li>Select the domain from the table.</li> <li>Under General configuration, choose Enable at the Access Proxy label.     </li> <li> <p>On the Create access proxy page, under Public access proxy, you must select TWO subnets for Public Subnets, named <code>LogHubVPC/DefaultVPC/publicSubnet1</code> and <code>LogHubVPC/DefaultVPC/publicSubnet2</code>.</p> </li> <li> <p>Choose a Security Group of the ALB in Public Security Group, named <code>LogHub-ProxySecurityGroup-xxx</code>.</p> </li> <li> <p>Input the following Domain Name:  <pre><code>fakename.workshop.log-hub.solutions.aws.dev\n</code></pre></p> <p>Notice that: This Domain Name is a fake one and we will do nothing for it in this workshop. For real use case, customers will need to enter a real Domain name, which will be used to grant access via public.</p> </li> <li> <p>Choose the associated Load Balancer SSL Certificate which applies to the domain name.</p> </li> <li> <p>Choose the Nginx Instance Key Pair Name. And check if the page looks like this graph below:     </p> </li> <li>Choose Create.      The above shows that proxys are creating now! </li> </ol> <p>It will take 10 minutes to fully deploy the proxy, so please wait until the creating status changes to a link like that:</p> <p></p> <p>So, please do not proceed until it's done.</p> <p>Note</p> <p>Please do not click the Link. We will access the proxy through ALB directly in the next section. </p>"},{"location":"workshop/domain-management/import-domain/","title":"Import AOS domain","text":"<p>Estimated time: 2 minutes</p> <p>Log Hub solution is built on top of Amazon OpenSearch Service. In this section, you will import an existing OpenSearch domain. </p> <ol> <li>In the navigation panel, under Clusters, choose Import OpenSearch Domain. </li> <li> <p>On Select domain page, choose a domain from the dropdown list. The dropdown list will only list domain in the same region as the Log Hub solution using VPC.</p> <p>We choose workshop-os, which is the OpenSearch we created in Deployment section using CloudFormation.</p> </li> <li> <p>Choose Next.</p> </li> <li>On Configure network page, under Network creation,     choose Automatic.    </li> <li>Choose Next.</li> <li>We can skip the Tags now, choose Import.</li> <li>We have successfully imported a OpenSearch Domain: </li> </ol>"},{"location":"workshop/domain-management/import-domain/#what-has-been-automatically-done-for-me","title":"What has been automatically done for me?","text":"<p>Firstly, a VPC peering connection is automatically established between your Workshop-Demo-VPC and your LogHub-VPC.</p> <p>Secondly, security groups and detailed routing mechanisms are automatically inserted into your Workshop-Demo-VPC. </p> <p>Info</p> <p>Let's take a look at the architecture for VPC peering:</p> <p></p> <p>As we can see, we need to peer the VPC which contains the log processors (10.255.0.0/16) with the OpenSearch VPC (10.0.0.0/16), and enable processors to go through the workshop demo VPC so as to process logs for us.</p>"},{"location":"workshop/log-analytics-pipelines/application-log/","title":"Ingest Application Logs via Log Hub Console","text":"<p>Estimated time: 10 minutes</p> <p>Log Hub supports ingest AWS Service logs and application (e.g. Nginx, Apache HTTP Server) logs.  This section will guide you to ingest the access logs of a group of Spring Boot servers which you have deployed in Deploy Demo Web Site.</p> <p>In this section, you will learn how to install log agents on selected instances, define log format and ingest logs on a single web console. The following is the architecture diagram. </p>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-application-log-pipeline","title":"Create application log pipeline","text":"<ol> <li> <p>Go to LogHub Web Console, choose Application Log in Log Analytics Pipelines section     </p> </li> <li> <p>Click Create a pipeline and type in the following parameters:</p> <ul> <li>Index name: <code>app-pipe</code></li> <li>Shard number: <code>2</code></li> <li>Enable auto scaling?: <code>No</code> </li> </ul> </li> <li>Click Next, and choose the AOS domain as workshop-os, remain other parameters and select Next, then Create </li> <li>Then you can see the pipeline is in Creating status.</li> </ol>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-ec2-policy","title":"Create EC2 policy","text":"<p>If we want to enable servers to be able to stream logs to kinesis, EC2 policies are needed! So let's do some quick steps and create a new EC2 policy for your Demo Website spring servers.</p> <p>Warning</p> <p>Please wait until the application pipeline status changes to <code>Active</code>!</p> <p></p> <ol> <li> <p>On the Log Hub Console, select the pipeline name and view the details of that log pipeline</p> </li> <li> <p>Go to Permission tab and copy the provided JSON policy </p> </li> <li> <p>Go to AWS Console &gt; IAM &gt; Policies on the left column</p> </li> <li> <p>Click Create Policy, choose JSON and replace all the content inside the text block. Remember to substitute <code>&lt;YOUR ACCOUNT ID&gt;</code> with your true account id! Please refer to the graph below: </p> </li> <li> <p>Click Next, Next, then type in the name for this policy, example name: <code>loghub-ec2-policy</code> </p> </li> <li> <p>Choose Create policy</p> </li> <li> <p>Go to AWS Console &gt; EC2, choose one of the instance which name is <code>LoghubWorkshop/workshopASG</code>, select Security tab and click the IAM Role link</p> </li> <li> <p>Click Add permissions &gt; Attach policies, and attach that newly created policy to this role</p> </li> </ol>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-instance-group","title":"Create Instance Group","text":"<ol> <li> <p>Go to LogHub Web Console, choose Instance Group on the left side of the page</p> </li> <li> <p>Click Create an instance group, now you can see two instances on the list:</p> </li> </ol> <p>Note</p> <p>All the instances with ssm agent will come up in this list, if there are non-relevant instances, please search for Instances ID in AWS Console first. Choosing the wrong instances might cause no data in dashboard.</p> <p> 3. Select both of the instances and click Install log agent, the agent installation process will start. We use fluent-bit as log agent in LogHub. Please wait until the installation complete, it will show Online in Pending Status column:  4. Then we can type in the name for instance group  and click Create. </p> <p>The instance group is successfully created.</p>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-spring-log-config","title":"Create Spring log config","text":"<ol> <li> <p>Go to Log Hub Console, choose Log Config on the left most side of the page</p> </li> <li> <p>Click Create a log config, type in the Config Name like: <code>spring-config</code>.</p> </li> <li> <p>Choose the log type as <code>Multi-line Text</code>.</p> <p>Choose the Parser as <code>Java-Spring Boot</code>.</p> </li> <li> <p>Copy paste the following log format in Log Format text box:  <pre><code>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%thread] %logger : %msg%n\n</code></pre></p> </li> <li> <p>Copy and paste the following sample log into the Sample Log box:      <pre><code>2022-02-18 10:32:26.400 ERROR [http-nio-8080-exec-1] org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.ArithmeticException: / by zero] with root cause\njava.lang.ArithmeticException: / by zero\n     at com.springexamples.demo.web.LoggerController.logs(LoggerController.java:22)\n     at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n     at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke\n</code></pre>      Click Parse Log, you can see the following attributes:      </p> <p>This means that your sample logs has been successfully parsed base on input log format.</p> </li> <li> <p>Click Save.</p> </li> </ol> <p>We have successfully created a multi-line Spring Boot log config.</p>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-log-ingestion","title":"Create log ingestion","text":"<p>Note</p> <p>Please make sure the application log pipeline is in Active status, before proceeding this section.</p> <ol> <li> <p>Find the application log pipeline you just created, by clicking it's name, we can enter the detailed page: </p> </li> <li> <p>Click Create an Ingestion, choose From Instance Group. Then, select Choose exists, click Next</p> </li> <li> <p>Select the instance group you have just created and click Next</p> </li> <li> <p>Type in the following log path: <pre><code>/tmp/springboot-sf4j-logback.log\n</code></pre></p> </li> <li> <p>Select Choose exists and  choose spring-config. The rest parameters will be auto filled for you.</p> </li> <li> <p>Click Next, then click Create</p> </li> </ol> <p>We have successfully created one ingestion for Spring Boot Logs.</p>"},{"location":"workshop/log-analytics-pipelines/application-log/#create-spring-boot-multi-line-logs","title":"Create Spring Boot Multi-line Logs","text":"<p>Let's go back to your Workshop Demo Website again, go to the detail page of Funny Moto. </p> <ol> <li> <p>Click Add To Cart button.</p> </li> <li> <p>Status Code 500 will show up, that's what we expected:  We are generating Spring Boot exceptions in the back-end server.</p> </li> </ol> <p>You can do these two steps multiple times to create more java multi-line logs.</p>"},{"location":"workshop/log-analytics-pipelines/application-log/#view-application-log-dashboard","title":"View Application Log Dashboard","text":"<p>We have created Spring Boot application log Pipeline, now let's go back to the OpenSearch Dashboard and have a look!</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Create Index Pattern</p> <p>Go to the location shown in the graph below and select Stack Management:  </p> <p>Select Index Patterns:  </p> <p>Select Create Index Pattern:  </p> <p>Type in <code>app-pipe-*</code> and click Next step &gt;:  </p> <p>Select time as time field and click Create index pattern:  </p> </li> <li> <p>Go to the location shown in the graph below:      </p> <p>Click app-pipe at the location below:  </p> </li> <li> <p>You can find the original Spring boot logs:</p> <p></p> </li> </ol> <p>We have completed all the steps of creating an application log pipeline.</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/","title":"Ingest AWS Service Logs via Log Hub Console","text":"<p>Log Hub provides two ways to ingest AWS Services logs, via Log Hub console or CloudFormation Stack. In this section, you will learn how to ingest Amazon CloudFront logs and RDS/Aurora MySQL logs using the Log Hub console.</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#cloudfront-logs","title":"CloudFront Logs","text":"<p>Estimated time: 12 minutes</p> <p>CloudFront Standard Logs provide detailed records about every request that\u2019s made to a distribution. In this chapter, you will learn how to ingest CloudFront access logs into Amazon OpenSearch service and build up dashboards. By following the steps, Log Hub will create the architecture in your AWS account: </p> <ol> <li>Go to Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose AWS Service Log.</li> <li>Choose the Create a log ingestion button.    </li> <li>In the AWS Services section, choose Amazon CloudFront.</li> <li>Choose Next.</li> <li>Under Specify settings, choose Automatic for CloudFront logs enabling. The automatic mode will detect the CloudFront log location automatically.</li> <li>For Automatic mode, choose the CloudFront distribution from the dropdown list.    You just need to choose the one named LogHub-Workshop Assets </li> <li>Choose Next.</li> <li> <p>In Log Processing page, we can select multiple Ingested fields and Enriched fields. Please select all the enriched fields by clicking <code>Location -optional</code> and <code>OS/User Agent -optional</code> The location enriched functions will be demonstrated in the next section.        </p> </li> <li> <p>Click Next.</p> </li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain. Remain the other part of the page unchanged.</li> <li>Choose Next.</li> <li>Choose Create.</li> </ol> <p>You can view the status of the stack in the LogHub Web console:  Status column shows creating means that the log pipeline is being created. </p> <p>Hold on!</p> <p>Before proceed to the next step, please wait until the pipeline status changes to Active. You can click the refresh button to get updated status.</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#create-cloudfront-fake-logs","title":"Create CloudFront fake logs","text":"<p>After the pipeline status changes to Active, we can go to the Workshop Demo Website and start creating some CloudFront fake logs. </p> <p>Note</p> <p>We are simulating a real use case from a customer whose e-commercial website is frequently being visited, so fake logs can help us to better understand customer's business situation.</p> <p>Firstly, go to the Workshop Demo Website, click Generate Logs, which is on the right-upper corner. Then click Generate CloudFront Logs.</p> <p></p> <p>Wait for a few seconds until it shows the following pop-up:</p> <p></p> <p>We have finished the log generation step!</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#view-cloudfront-log-dashboard","title":"View Cloudfront Log Dashboard","text":"<p>Since we have created the CloudFront Log Pipeline and generated fake log, now let's go back to the OpenSearch Dashboard and have a look!</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Go to the location shown in the graph below, you can find the CloudFront dashboard have already been imported for you, which name is <code>xxxxxxxxxxxxx-cloudfront-dashboard</code>. Click it and you can view all the details by yourself: </p> </li> </ol> <p>The CloudFront dashboard should look like this: </p> <p>So far, we have successfully created a service pipeline for CloudFront Service, and we are able to get insight of the dashboard in OpenSearch.</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#rdsaurora-mysql-logs","title":"RDS/Aurora MySQL Logs","text":"<p>Estimated time: 10 minutes</p> <p>In this section, we will describe how to ingest logs from Amazon RDS into Amazon OpenSearch service and build up  visualization dashboards. By following the steps, Log Hub will create the architecture in your AWS account: </p> <ol> <li>Go back to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose AWS Service Log.</li> <li>Choose the Create a log ingestion button.    </li> <li>In the AWS Services section, choose Amazon RDS.</li> <li>Choose Next.</li> <li>Under Specify logs settings, choose Automatic.</li> <li>Choose the RDS cluster from the dropdown list.    We choose <code>MySQL-workshop-db</code>.        Make sure you select the Audit log option!</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain. Remain the other part of the page unchanged.</li> <li>Choose Next.</li> <li>Choose Create.</li> </ol> <p>You can view the status of the stack in the LogHub Web console.</p> <p>Status column shows creating means that the log pipeline is being created. </p> <p>Hold on!</p> <p>Please wait until the status change to \"Active\" before proceeding.</p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#generate-slow-query-logs","title":"Generate slow query logs","text":"<p>Now we can go to the Workshop Demo Website and start creating some RDS logs.</p> <p>Firstly, go back to Workshop Demo Website home again, we can see three products listed on the Website. We click the View Detail button under Funny Moto.</p> <p></p> <p>Note</p> <p>The product details will show up very slowly. Because we are generating slow query logs here.</p> <p>That's what we expected, because we are generating slow query logs now. </p>"},{"location":"workshop/log-analytics-pipelines/cloudfront-log/#view-rds-log-dashboard","title":"View RDS Log Dashboard","text":"<p>We can now leave the website behind and go to OpenSearch Dashboard to take a peek.</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Go to the location shown in the graph below, you can find the RDS dashboard have already been imported for you, which name is <code>workshop-db-rds-dashboard</code>. Click it and you can view all the details by yourself: </p> </li> </ol> <p>The RDS dashboard should look like this: </p> <p>Congratulations! We have created the service log pipeline for RDS successfully.</p>"},{"location":"workshop/log-analytics-pipelines/eks-log/","title":"Ingest EKS Pod Logs via Log Hub Console","text":""},{"location":"workshop/log-analytics-pipelines/eks-log/#create-nginx-log-config","title":"Create Nginx log config","text":"<ol> <li> <p>Go to Log Hub Console, choose Log Config on the left most side of the page</p> </li> <li> <p>Click Create a log config, type in the Config Name like: <code>nginx-config</code>.</p> </li> <li> <p>Choose the log type as <code>Nginx</code>.</p> </li> <li> <p>Copy paste the following log format in Log Format text box:  <pre><code>log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n'$status $body_bytes_sent \"$http_referer\" '\n'\"$http_user_agent\" \"$http_x_forwarded_for\"';\n</code></pre></p> </li> <li> <p>Copy and paste the following sample log into the Sample Log box:      <pre><code>127.0.0.1 - - [24/Dec/2021:01:27:11 +0000] \"GET / HTTP/1.1\" 200 3520 \"-\" \"curl/7.79.1\" \"-\"\n</code></pre>      Click Parse Log</p> </li> <li> <p>Click Save.</p> </li> </ol> <p>We have successfully created a multi-line Spring Boot log config.</p>"},{"location":"workshop/log-analytics-pipelines/eks-log/#import-an-eks-cluster","title":"Import an EKS Cluster","text":"<ol> <li>Go to the Log Hub Console.</li> <li>In the left sidebar, under Log Source, choose EKS Cluster.</li> <li>Click the Import a Cluster button.</li> <li>Choose the loghub-workshop-eks.</li> <li>Select DaemonSet as log agent's deployment pattern. </li> <li>Choose Next.</li> <li>Choose the AOS domain as workshop-os.</li> <li>Configure the Network the EKS cluster and OpenSearch<ul> <li>During the pre-requisites, we have created the EKS in the same VPC of OpenSearch. So, we can skip the VPC peering in this workshop. But we still need to update the security group.</li> <li>Go to AWS Console &gt; OpenSearch</li> <li>Copy the value of <code>Security group</code>. Then click the security group to view details       </li> <li>Edit the inbound rule of the security group       </li> <li>Allow <code>All Traffic</code> from the <code>Security group</code> you copied above. Save the rules       </li> <li>Go back to the Log Hub console, and check the checkbox that you have completed the \"Network Configuration\".</li> </ul> </li> <li>Choose Next.</li> <li> <p>Choose Create.</p> <p></p> </li> </ol>"},{"location":"workshop/log-analytics-pipelines/eks-log/#create-eks-pod-log-ingestion","title":"Create eks pod log ingestion","text":"<ol> <li>Click the EKS Cluster loghub-workshop-eks that has been imported above.</li> <li>Go to App Log Ingestion tab and click Create an Ingestion. </li> <li>In Specify Pipeline Settings page, Select Create new<ul> <li>Index Prefix: <code>eks-nginx</code></li> <li>Click Next</li> </ul> </li> <li>In Specify log config page:</li> <li>Type in the following log path: <pre><code>/var/log/containers/app-nginx-demo*nginx-ns*\n</code></pre>      And <ul> <li>Select nginx-config from Log Config dropdown.</li> <li>Click Next</li> </ul> </li> <li>Click Create</li> <li> <p>Wait the App Log Ingestion Status as Created</p> <p></p> </li> </ol>"},{"location":"workshop/log-analytics-pipelines/eks-log/#deploy-fluent-bit-agent-to-eks","title":"Deploy fluent-bit agent to EKS","text":"<p>Waring</p> <p>Please follow the workshop guide to deploy the fluent-bit agent</p> <ol> <li> <p>Click the DaemonSet Guide tab</p> <p></p> </li> <li> <p>Deploy fluent-bit log agent as DaemonSet</p> <ul> <li>Go to the Cloud9 workspace created in Pre-request</li> <li>Create a file fluent-bit-logging.yaml and copy&amp;paste the yaml generated in LogHub console</li> <li>run the command below in terminal <pre><code>kubectl apply -f fluent-bit-logging.yaml\n</code></pre></li> </ul> </li> <li>Make sure your fluent-bit is running <pre><code>kubectl get pods -n logging\n</code></pre> </li> </ol>"},{"location":"workshop/log-analytics-pipelines/eks-log/#generate-nginx-pod-logs","title":"Generate Nginx Pod Logs","text":"<ol> <li> <p>Run the command, and you will find the load balancer at LoadBalancer Ingress in response <pre><code>kubectl describe service nginx-service -n nginx-ns\n</code></pre> </p> </li> <li> <p>In a browser, enter the load balancer address, and you should see Welcome to nginx</p> <p></p> </li> </ol> <p>This will generate access log. Feel free to generate more by refreshing the page.</p>"},{"location":"workshop/log-analytics-pipelines/eks-log/#view-the-nginx-log-dashboard","title":"View the Nginx Log Dashboard","text":"<p>We can now leave the website behind and go to OpenSearch Dashboard to take a peek.</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Go to the location shown in the graph below, you can find the RDS dashboard have already been imported for you, which name is <code>eks-nginx-dashboard</code>. Click it and you can view all the details by yourself: </p> </li> </ol> <p>The dashboard should look like this: </p> <p>We have completed all the steps of creating an eks pod log pipeline.</p>"},{"location":"workshop/log-analytics-pipelines/elb-log/","title":"Ingest Logs via CloudFormation Stack","text":"<p>Estimated time: 10 minutes</p> <p>In addition to the Log Hub console, Log Hub also allows customers to ingest logs by provisioning a standalone CloudFormation template.  These templates are super useful when</p> <ul> <li>The customer has limited log types, for example the customer only one type of logs to analyze.</li> <li>The customer wants to integrate with the Infrastructure-as-Code technology, and they can reuse the CloudFormation templates.</li> </ul> <p>In this chapter, you will learn how to ingest ELB access logs into Amazon OpenSearch service and build up dashboards. ELB Access logs provides access logs that capture detailed information about requests sent to your load balancer. ELB publishes a log file for each load balancer node every 5 minutes.</p>"},{"location":"workshop/log-analytics-pipelines/elb-log/#enable-elb-access-logs","title":"Enable ELB Access Logs","text":"<ol> <li>Go to AWS Console &gt; EC2 &gt; Load Balancers on the left column of the page </li> <li> <p>Select the balancer which name starts with <code>Works-works-</code></p> </li> <li> <p>Under Description tab, we can find Attributes section, select Edit attributes</p> </li> <li> <p>Please enable AccessLogs and create a new s3 location to store ELB, please save the name of s3 for later usage. Also, select Create this location for me. The suggested name is:  <pre><code>&lt;your-login&gt;-loghub-workshop-logging-bucket/elb\n</code></pre> Please double-check if you have selected all the following items: </p> </li> <li>Click Save.</li> </ol>"},{"location":"workshop/log-analytics-pipelines/elb-log/#create-log-ingestion-using-cloudformation","title":"Create log ingestion using CloudFormation","text":"<ol> <li> <p>Log in the AWS Management Console and select the button to launch the <code>LogHub-ELBLog</code> AWS CloudFormation template.</p> <p></p> </li> <li> <p>Click Next and fill in the parameters required:</p> Parameter Default Description Log Bucket Name <code>&lt;Requires input&gt;</code> The S3 bucket stores ELB log, which we created in adding ELB attributes: <code>YOURLOGIN-loghub-workshop-logging-bucket</code> Log Bucket Prefix <code>&lt;Requires input&gt;</code> Type in elb Engine Type OpenSearch Choose OpenSearch. OpenSearch Domain Name <code>&lt;Requires input&gt;</code> Type in workshop-os OpenSearch Endpoint <code>&lt;Requires input&gt;</code> The OpenSearch endpoint URL. You can find it inside the Log Hub Portal: AWS Console &gt; Cloudformation &gt; Stacks &gt; WorkshopDemo &gt; Ouputs &gt; opensearchDomain Index Prefix <code>&lt;requires input&gt;</code> Type in workshop Create Sample Dashboard Yes Choose Yes this time. VPC ID <code>&lt;requires input&gt;</code> Select the VPC which name starts with LogHub/LogHubVpc/DefaultVPC Subnet IDs <code>&lt;requires input&gt;</code> Select TWO private subnets which names are LogHub/LogHubVpc/DefaultVPC/privateSubnet1 and LogHub/LogHubVpc/DefaultVPC/privateSubnet2 Security Group ID <code>&lt;requires input&gt;</code> Select the Security Group which name start with LogHub-ProcessSecurityGroup- S3 Backup Bucket <code>&lt;requires input&gt;</code> <code>YOURLOGIN-loghub-workshop-logging-bucket</code> Number Of Shards 5 Number of shards to distribute the index evenly across all data nodes, keep the size of each shard between 10-50 GiB. Number of Replicas 1 The number of days required to move the index into warm storage, this is only effecitve when the value is &gt;0 and warm storage is enabled in OpenSearch. Days to Warm Storage 0 Number of replicas for OpenSearch Index. Each replica is a full copy of an index. Days to Cold Storage 0 The number of days required to move the index into cold storage, this is only effecitve when the value is &gt;0 and cold storage is enabled in OpenSearch. Days to Retain 0 The total number of days to retain the index, if value is 0, the index will not be deleted. <p>Your parameters should look mostly like this:</p> <p></p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p> <p>We have successfully created a service log pipeline for ELB from Cloudformation, and congrats on finishing all the sections in this workshop! </p>"},{"location":"workshop/log-analytics-pipelines/multi-line-dashboard/","title":"Multi line dashboard","text":""},{"location":"workshop/log-analytics-pipelines/multi-line-dashboard/#view-multi-line-log-dashboard","title":"View Multi-line Log Dashboard","text":"<p>We have created the multi-line application Pipeline, now let's go back to the OpenSearch Dashboard and have a look!</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Go to the location shown in the graph below, you can find the Multi-line application dashboard, which have already been imported for you, which name is <code>-dashboard</code>. Click it and you can view all the details by yourself: </p> </li> </ol>"},{"location":"workshop/log-analytics-pipelines/rds-log/","title":"Rds log","text":""},{"location":"workshop/log-analytics-pipelines/rds-log/#rdsaurora-mysql-logs","title":"RDS/Aurora MySQL Logs","text":"<p>Estimated time: 10 minutes</p> <p>In this section, we will describe how to ingest logs from Amazon RDS into Amazon OpenSearch service and build up  visualization dashboards. By following the steps, Log Hub will create the architecture in your AWS account: </p> <ol> <li>Sign in to the Log Hub Console.</li> <li>In the navigation pane, under Log Analytics Pipelines, choose Service Log.</li> <li>Choose the Create a log ingestion button.    </li> <li>In the AWS Services section, choose Amazon RDS.</li> <li>Choose Next.</li> <li>Under Specify logs settings, choose Automatic.</li> <li>Choose the RDS cluster from the dropdown list.    We choose <code>MySQL-workshop-db</code>.        Make sure you select the Audit log option!</li> <li>Choose Next.</li> <li>In the Specify OpenSearch domain section, select an imported domain for Amazon OpenSearch domain. Remain the other part of the page unchanged.</li> <li>Choose Next.</li> <li>Choose Create.</li> </ol> <p>You can view the status of the stack in the LogHub Web console.</p> <p>Status column shows creating means that the log pipeline is being created. </p> <p>Hold on!</p> <p>Please wait until the status change to \"Active\" before proceeding.</p>"},{"location":"workshop/log-analytics-pipelines/rds-log/#generate-slow-query-logs","title":"Generate slow query logs","text":"<p>Now we can go to the Workshop Demo Website and start creating some RDS logs.</p> <p>Firstly, go back to Workshop Demo Website home again, we can see three products listed on the Website. We click the View Detail button under Funny Moto.</p> <p></p> <p>Notice that: The product details will show up very slowly. Because we are generating slow query logs here.</p> <p>That's what we expected, because we are generating slow query logs now. </p>"},{"location":"workshop/log-analytics-pipelines/rds-log/#view-rds-log-dashboard","title":"View RDS Log Dashboard","text":"<p>We can now leave the website behind and go to OpenSearch Dashboard to take a peek.</p> <ol> <li> <p>Open the Dashboard page in your browser.</p> </li> <li> <p>Go to the location shown in the graph below, you can find the RDS dashboard have already been imported for you, which name is <code>workshop-db-rds-dashboard</code>. Click it and you can view all the details by yourself: </p> </li> </ol> <p>The RDS dashboard should look like this: </p> <p>Congratulations! We have created the service log pipeline for RDS successfully.</p>"}]}